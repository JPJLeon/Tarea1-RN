{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1dvfXS9G-jF"
   },
   "source": [
    "### <img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales - 2019-2 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 1  </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "* Manipulaciones en pandas y numpy, preprocesamientos\n",
    "* Redes Densas Feed Forward\n",
    "* Regularización y Dropout\n",
    "* Vanishing Gradient y Skip Connections\n",
    "* Learn Rate Decay\n",
    "* Optimizadores\n",
    "* Redes Convolucionales\n",
    "* Image Data Agumentation\n",
    "\n",
    "\n",
    "**Formalidades**  \n",
    "* Equipos de trabajo de 2 personas (*Ambos estudiantes deben estar preparados para presentar la tarea el día de la entrega*)\n",
    "* El entregable debe ser un _Jupyter Notebook_ incluyendo los códigos utilizados, los resultados, los gráficos realizados y comentarios. Debe seguir una estructura similar a un informe (se debe introducir los problemas a trabajar, presentar los resultados y discutirlos), se penalizará fuertemente ausencia de comentarios, explicaciones de gráficos, _etc_. Si lo prefiere puede entregar un _Jupyter Notebook_ por pregunta o uno por toda la tarea, con tal de que todos los entregables esten bien identificados y se encuentren en el mismo repositorio de _Github_.\n",
    "* Se debe preparar una presentación del trabajo realizado y sus hallazgos. El presentador será elegido aleatoriamente y deberá apoyarse en el _Jupyter Notebook_ que entregarán. \n",
    "* Formato de entrega: envı́o de link del repositorio en _Github_, al correo electrónico del ayudante (<alvaro.valderrama.13@sansano.usm.cl>), en copia al profesor (<cvalle@inf.utfsm.cl>).   Especificar el siguiente asunto: [INF-395/477-2019 Tarea 1]. Invitar como colaborador al usuario de github \"avalderr\" para poder acceder al repositorio en caso de ser privado.\n",
    "* Fecha de entrega y presentaciones: 22 de Noviembre. Hora límite de entrega: 23:59. Cualquier _commit_ luego de la hora límite no será evaluado. Se realizará descuento por atrasos en envío del mail igualmente.  \n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "La tarea se divide en tres partes:\n",
    "\n",
    "* 1 - Redes Feed Forward para Airbnb\n",
    "* 2 - Reconocimiento de Imagenes en CIFAR10    \n",
    "* 3 - Reconocimiento de frutas y verduras\n",
    "\n",
    "La tarea tiene ejemplos de códigos con los cuales pueden guiarse en gran parte, sin embargo, solo son guias y pueden ser creativos al momento de resolver la tarea. Soluciones creativas o elegantes serán valoradas. También en algunas ocaciones se hacen elecciones arbitrarias, ustedes pueden realizar otras elecciones con tal de que haya una pequeña justificación de por qué su elección es mejor o equivalente.\n",
    "Recuerden intercalar su código con *comentarios* en celdas _Markdown_, con los comentarios de la pregunta y con cualquier analisis, fórmula (en $ \\LaTeX $) o explicación que les parezca relevante para justificar sus procedimientos. *No respondan las preguntas en comentarios en el código*.\n",
    "Noten que en general cuando se les pide elegir algo o proponer algo no se evaluará tanto la elección en si. En cambio la argumentación detrás de la elección será lo más ponderado.\n",
    "Si algun modelo se demora demasiado en correr en su maquina, no olvide que puede correr _Jupyter Notebooks_ en _Collab_ de Google, incluso con la opción de aceleración con GPU (particularmente útil para los modelos más grandes), esto puede ser relevante para las maquinas más lentas al momento de realizar exploraciones con _K-folds_ o las redes más grandes. Existe también la posibilidad de utilizar _Google Cloud Plataform_, donde tienen 300 dolares de prueba por un año y pueden comprar tiempo de procesamiento en maquinas aceleradas con GPU; maquinas ya configuradas para _deep leraning_ pueden encontrarse en el _Marketplace_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZhRK47TDG-jK"
   },
   "source": [
    "_________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HkYuH1gEG-mV"
   },
   "source": [
    "# 3 - Reconocimiento de frutas y verduras\n",
    "\n",
    "Si bien CIFAR10 es una buenisima herramienta para introducirse a las redes neuronales convolucionales, sin embargo varias características de CIFAR10 lo hacen estar un poco alejado de la realidad del reconocimiento de imagenes hoy en día. Por ejemplo, como pudieron notar en la pregunta anterior, la resolución de $32\\times 32$ es a penas suficiente para reconocer el objeto. Además 10 clases es relativamente poco comparado con los modelos del estado del arte, que obtienen resultado decentes en problemas con miles de categorías. Otra cosa que podrían haber notado, es que las redes entrenadas no tenían mucho problema en diferenciar clases \"más\" distintas que otras (por ejemplo, que cree que hubiera pasado si hubieramos truncado el _dataset_ para solo contener la clase caballo y avión, ¿cree que los desempeños serían los mismos?), mientras que en categorías más \"cercanas\" (como perro, gato y caballo) los desempeños eran relativamente peores. Es por esto que muchos _datasets_ utilizados en el estado del arte incluyen no solo variedad entre sus categorías, si no tambien categorías similares, que podrían agruparse en categorías de mayor gerarquía incluso. \n",
    "\n",
    "En esta pregunta, intentaremos de realizar un analisis sobre un _dataset_ un poco más realista, donde encontrará algunos de los problemas asociados a trabajar con gran número de imagenes. Originalmente se consideró trabajar con uno de los _dataset_ usuales en el estado del arte, el Caltech256, sin embargo por motivos de tiempo se prefirió este _dataset_ donde las imagenes vienen todas en el mismo formato ($100\\times 100$) y no tendrémos que realizar preprocesamiento más acabados como aquellos que necesitaríamos con Caltech. El _dataset_ en cuestión corresponde a un problema de reconocimiento de frutas y verduras, disponible en el siguiente link de _kaggle_ https://www.kaggle.com/moltean/fruits o en el siguiente repositorio https://github.com/Horea94/Fruit-Images-Dataset. Este _dataset_ contiene miles de imagenes de 120 frutas y verduras diferentes clasificadas por variedades. Intentaremos resolver este problema utilizando las 120 clases, notando que si bien el hecho de acortarse a un campo semantico en particular (i.e. frutas y verduras) permitirá que nuestro algoritmo se especialice más en la detección de las diferencias entre sus categorías, también implica que las diferencias seran más pequeñas que lo que serían en un _dataset_ más variado. Luego intentaremos resolver el problema utilizando categorías más amplias, donde no se distinga entre variedades de la misma fruta, y veremos en qué tarea tenemos mejor desempeño. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DTu0O8PG-mV"
   },
   "source": [
    "### 3.a Carga y preprocesamiento de Imagenes\n",
    "\n",
    "Lea la descripción del dataset para familiarizarse con la estructura donde se guardan las imagenes y como deberemos cargarlas recordando la categoría de cada una. \n",
    "\n",
    "Cargue todas las imagenes junto con sus categorías a un _DataFrame_. Note que esto puede tomar bastante tiempo por la cantidad de imagenes. El código propuesto utiliza la librería Pillow, sin embargo pueden usar el método que prefieran, incluso pueden no basarse para nada en el código propuesto. Aprovecharemos la separación de _Training_ y _Test_ de _dataset_, y utilizaremos el primero para entrenamiento y el segundo para validación. \n",
    "\n",
    "Esta vez optaremos por dejar los valores entre $[-127, 128]$ para poder utilizar un encoding `uint8` en `numpy` lo cual reducirá drásticamente el uso de memoria al momento de cargar los datos. Separe luego los arrays de inputs y outputs de nuestro modelo, sin olvidar transformar las categorías del _target_ a _one hot vector_ como aprendió en el resto de la tarea. \n",
    "\n",
    "Si lo desea, podría ser recomendable guardar los datos preprocesados utiizando la función `np.save` para ahorrarse el tiempo de computo que requiere esta operación en caso de que tuviera que correr varias veces esta pregunta. \n",
    "\n",
    "Visualice alguna de las imagenes en `image_list`. Verifique que los datos se hayan cargado bien viendo los `shape` de los arrays o con algun otro indicador que le dé algo de confianza. Utilice `matplotlib.pyplot.imshow` con algunos ejemplos de los _arrays_ para verificar que no se haya perdido información y que la carga de datos se realizó correctamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CASgyQ11G-mW",
    "outputId": "996ed86a-ae1a-4157-b0e3-d09c19dfa045",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd67adb6da0>"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import PIL.Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base_path = os.getcwd()\n",
    "data_path_train = os.path.join(base_path, \"fruits-360_dataset\", \"fruits-360\", \"Training\") \n",
    "data_path_test = os.path.join(base_path, \"fruits-360_dataset\", \"fruits-360\", \"Test\") \n",
    "categories_train = os.listdir(data_path_train)\n",
    "categories_test = os.listdir(data_path_test)\n",
    "\n",
    "image_list_train = []\n",
    "image_list_test = []\n",
    "\n",
    "df_train = pd.DataFrame(columns=['x','y'])\n",
    "for cat in categories_train:\n",
    "    image_files = os.listdir(os.path.join(data_path_train, cat))\n",
    "    for image_path in image_files:\n",
    "        if '.jpg' in image_path: # to avoid unpleseant surprises\n",
    "            full_path = os.path.join(data_path_train, cat, image_path)\n",
    "            im = PIL.Image.open(full_path)\n",
    "            arr = np.asarray(im)\n",
    "            df_train = df_train.append({'x':arr, 'y':cat}, ignore_index=True)\n",
    "    image_list_train.append(im)\n",
    "\n",
    "df_test = pd.DataFrame(columns=['x','y'])\n",
    "for cat in categories_test:\n",
    "    image_files = os.listdir(os.path.join(data_path_test, cat))\n",
    "    for image_path in image_files:\n",
    "        if '.jpg' in image_path: # to avoid unpleseant surprises\n",
    "            full_path = os.path.join(data_path_test, cat, image_path)\n",
    "            im = PIL.Image.open(full_path)\n",
    "            arr = np.asarray(im)\n",
    "            df_test = df_test.append({'x':arr, 'y':cat}, ignore_index=True)\n",
    "    image_list_test.append(im)\n",
    "\n",
    "plt.imshow(image_list_train[0])\n",
    "plt.imshow(image_list_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TxmHYlzbga-m",
    "outputId": "5a54af78-d306-48e9-8acc-38be4518c5c7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marimo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/marimo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/marimo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/marimo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/marimo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/marimo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60498, 100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(20622, 100, 100, 3)\n",
      "(100, 100, 3)\n",
      "(60498, 120)\n",
      "(120,)\n",
      "(20622, 120)\n",
      "(120,)\n",
      "[[[[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]]\n",
      "\n",
      "\n",
      " [[[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]]\n",
      "\n",
      "\n",
      " [[[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]]\n",
      "\n",
      "\n",
      " [[[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]]\n",
      "\n",
      "\n",
      " [[[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]]]\n",
      "[[[[127 128 128]\n",
      "   [127 128 128]\n",
      "   [127 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[127 128 128]\n",
      "   [127 128 128]\n",
      "   [127 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[127 128 128]\n",
      "   [127 128 128]\n",
      "   [127 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]]\n",
      "\n",
      "\n",
      " [[[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]]\n",
      "\n",
      "\n",
      " [[[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]]\n",
      "\n",
      "\n",
      " [[[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]]\n",
      "\n",
      "\n",
      " [[[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]\n",
      "\n",
      "  [[128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   ...\n",
      "   [128 128 128]\n",
      "   [128 128 128]\n",
      "   [128 128 128]]]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "Y_train = le.fit_transform(df_train['y'])\n",
    "Y_val = le.transform(df_test['y'])\n",
    "\n",
    "df_train['x'] = df_train['x'] - 127\n",
    "df_test['x'] = df_test['x'] - 127\n",
    "\n",
    "X_train = df_train['x'].values\n",
    "y_train = to_categorical(Y_train)\n",
    "X_val = df_test['x'].values\n",
    "y_val = to_categorical(Y_val)\n",
    "\n",
    "X_train = np.concatenate(X_train).reshape(60498, 100, 100, 3)\n",
    "X_val = np.concatenate(X_val).reshape(20622, 100, 100, 3)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train[0].shape)\n",
    "print(X_val.shape)\n",
    "print(X_val[0].shape)\n",
    "print(y_train.shape)\n",
    "print(y_train[0].shape)\n",
    "print(y_val.shape)\n",
    "print(y_val[0].shape)\n",
    "\n",
    "print(X_train)\n",
    "print(X_val)\n",
    "\n",
    "n_classes = y_train.shape[1]\n",
    "\n",
    "np.save('X_train.npy', X_train)\n",
    "np.save('X_val.npy', X_val)\n",
    "np.save('y_train.npy', y_train)\n",
    "np.save('y_val.npy', y_val)\n",
    "\n",
    "df_train = []\n",
    "df_test = []\n",
    "image_list_train = []\n",
    "image_list_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La carga de datos fue exitosa, y para el preprocesamiento y separación se dejan listos los 2 targets (fruta y fruta con categoría). Además de esto, un reshape es necesario para que keras pueda leer correctamente el input. Podemos ver al imprimir los shapes que está todo en orden. Todo este preprocesamiento se hizo de forma local para luego guardar los archivos de entrenamiento y correrlos en Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Is-TnidG-md"
   },
   "source": [
    "### 3.b Primera red\n",
    "Entrene una primera red que alcance un _accuracy_ sobre validación de a lo menos un $80\\%$\n",
    "\n",
    "Puede utilizar todos los conceptos aprendidos en la tarea anterior, aunque la recomendación es dejar _Data Augmentation_ para una última iteración, una vez ya hayamos encontrado una estructura que se comporte relativamente bien. Una arquitectura relativamente buena podria ser una basada en los bloques definidos en el item 2.e, con al rededor de 5 bloques. Si desea aumentar la profundidad de su red sientase libre utilizar otras estructuras con menos _MaxPool_. Note que la nueva base de datos permite crear redes más profundas de todas  formas, pues no se alcanza el limite impuesto por la dimensión de las imagenes tan pronto. \n",
    "\n",
    "Reporte el resultado de un par de redes, comentando por qué realizo ajustes a ellas. Note que estos entrenamiento implican calculos con grandes cantidades de datos y probablemente sea recomendarlo correrlos en _hardware_ acelerado por GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "colab_type": "code",
    "id": "6IQD2a47hThc",
    "outputId": "499e0dd2-4e00-44c3-bc26-2aa05aab86f4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#uploaded = files.upload()\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se monta la unidad de drive con el notebook, es una forma muy buena de usar archivos pesados en Colab, considerando que la velocidad de subida a Drive es mucho mayor que la velocidad de subida a Colab, y no tiene mayor demora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AvICyXtI_aNI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.load('drive/My Drive/Colab Notebooks/X_train.npy')\n",
    "X_val = np.load('drive/My Drive/Colab Notebooks/X_val.npy')\n",
    "y_train = np.load('drive/My Drive/Colab Notebooks/y_train.npy')\n",
    "y_val = np.load('drive/My Drive/Colab Notebooks/y_val.npy')\n",
    "y_train2 = np.load('drive/My Drive/Colab Notebooks/y_train2.npy')\n",
    "y_val2 = np.load('drive/My Drive/Colab Notebooks/y_val2.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se cargan los archivos en el notebook de Colab, sin mayor problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "colab_type": "code",
    "id": "Z_9pO70Ega-q",
    "outputId": "3ec5c935-8ab7-4458-8d95-903753cd0137",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60498 samples, validate on 20622 samples\n",
      "Epoch 1/40\n",
      "60498/60498 [==============================] - 70s 1ms/sample - loss: 1.2473 - categorical_accuracy: 0.6976 - val_loss: 1.2609 - val_categorical_accuracy: 0.7158\n",
      "Epoch 2/40\n",
      "60498/60498 [==============================] - 71s 1ms/sample - loss: 0.2355 - categorical_accuracy: 0.9288 - val_loss: 1.4774 - val_categorical_accuracy: 0.7376\n",
      "Epoch 3/40\n",
      "60498/60498 [==============================] - 70s 1ms/sample - loss: 0.1668 - categorical_accuracy: 0.9506 - val_loss: 1.0747 - val_categorical_accuracy: 0.8030\n",
      "Epoch 4/40\n",
      "60498/60498 [==============================] - 70s 1ms/sample - loss: 0.1205 - categorical_accuracy: 0.9658 - val_loss: 1.1359 - val_categorical_accuracy: 0.8061\n",
      "Epoch 5/40\n",
      "60498/60498 [==============================] - 68s 1ms/sample - loss: 0.1148 - categorical_accuracy: 0.9669 - val_loss: 1.1135 - val_categorical_accuracy: 0.7930\n",
      "Epoch 6/40\n",
      "60498/60498 [==============================] - 68s 1ms/sample - loss: 0.0986 - categorical_accuracy: 0.9738 - val_loss: 1.4042 - val_categorical_accuracy: 0.7928\n",
      "Epoch 7/40\n",
      "60498/60498 [==============================] - 69s 1ms/sample - loss: 0.0892 - categorical_accuracy: 0.9764 - val_loss: 1.5193 - val_categorical_accuracy: 0.7694\n",
      "Epoch 8/40\n",
      "60498/60498 [==============================] - 74s 1ms/sample - loss: 0.0710 - categorical_accuracy: 0.9808 - val_loss: 1.5539 - val_categorical_accuracy: 0.7741\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#Callback\n",
    "stopper = EarlyStopping('val_loss', 0.01, patience=5)\n",
    "\n",
    "#Arquitectura\n",
    "visible = Input(shape=(100, 100, 3), name='input')\n",
    "conv1 = Conv2D(128, kernel_size=3, activation='relu')(visible)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, kernel_size=3, activation='relu')(pool1)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(32, kernel_size=3, activation='relu')(pool2)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "flat = Flatten()(pool3)\n",
    "hidden1 = Dense(256, activation='relu')(flat)\n",
    "hidden2 = Dense(128, activation='relu')(hidden1)\n",
    "hidden3 = Dense(64, activation='relu')(hidden2)\n",
    "output = Dense(120, activation='softmax')(hidden3)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "adam = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=40,\n",
    "                    callbacks=[stopper],\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se entrena esta arquitectura de forma simple, la cual llega a 80% en validación en la época 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "colab_type": "code",
    "id": "0m88s7H9C9Ek",
    "outputId": "d32abbad-1e08-4359-c096-186116ed3a50",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60498 samples, validate on 20622 samples\n",
      "Epoch 1/40\n",
      "60498/60498 [==============================] - 76s 1ms/sample - loss: 2.8935 - categorical_accuracy: 0.3160 - val_loss: 1.4282 - val_categorical_accuracy: 0.6651\n",
      "Epoch 2/40\n",
      "60498/60498 [==============================] - 71s 1ms/sample - loss: 0.4422 - categorical_accuracy: 0.8649 - val_loss: 0.5981 - val_categorical_accuracy: 0.8579\n",
      "Epoch 3/40\n",
      "60498/60498 [==============================] - 68s 1ms/sample - loss: 0.1923 - categorical_accuracy: 0.9428 - val_loss: 0.4452 - val_categorical_accuracy: 0.8984\n",
      "Epoch 4/40\n",
      "60498/60498 [==============================] - 67s 1ms/sample - loss: 0.1382 - categorical_accuracy: 0.9613 - val_loss: 0.5819 - val_categorical_accuracy: 0.8832\n",
      "Epoch 5/40\n",
      "60498/60498 [==============================] - 74s 1ms/sample - loss: 0.1170 - categorical_accuracy: 0.9683 - val_loss: 0.5319 - val_categorical_accuracy: 0.8865\n",
      "Epoch 6/40\n",
      "60498/60498 [==============================] - 72s 1ms/sample - loss: 0.1088 - categorical_accuracy: 0.9713 - val_loss: 0.5433 - val_categorical_accuracy: 0.8964\n",
      "Epoch 7/40\n",
      "60498/60498 [==============================] - 72s 1ms/sample - loss: 0.0921 - categorical_accuracy: 0.9754 - val_loss: 0.5627 - val_categorical_accuracy: 0.8955\n",
      "Epoch 8/40\n",
      "60498/60498 [==============================] - 67s 1ms/sample - loss: 0.0935 - categorical_accuracy: 0.9772 - val_loss: 0.7048 - val_categorical_accuracy: 0.8875\n"
     ]
    }
   ],
   "source": [
    "#Arquitectura\n",
    "visible = Input(shape=(100, 100, 3), name='input')\n",
    "conv1 = Conv2D(128, kernel_size=3, activation='relu')(visible)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, kernel_size=3, activation='relu')(pool1)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(32, kernel_size=3, activation='relu')(pool2)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "flat = Flatten()(pool3)\n",
    "hidden1 = Dense(256, activation='relu')(flat)\n",
    "drop1 = Dropout(0.2)(hidden1)\n",
    "hidden2 = Dense(128, activation='relu')(drop1)\n",
    "drop2 = Dropout(0.2)(hidden2)\n",
    "hidden3 = Dense(64, activation='relu')(drop2)\n",
    "output = Dense(120, activation='softmax')(hidden3)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "adam = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=40,\n",
    "                    callbacks=[stopper],\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se procede a agregar Dropout a la arquitectura, logrando resultados aún mejores en validación casi llegando al 90% de accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 690
    },
    "colab_type": "code",
    "id": "rBZ6osWkC92l",
    "outputId": "f3ee94db-494c-4afa-ab34-a26ff17506c9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60498 samples, validate on 20622 samples\n",
      "Epoch 1/40\n",
      "60498/60498 [==============================] - 68s 1ms/sample - loss: 3.2778 - categorical_accuracy: 0.2875 - val_loss: 1.0060 - val_categorical_accuracy: 0.7681\n",
      "Epoch 2/40\n",
      "60498/60498 [==============================] - 72s 1ms/sample - loss: 0.8544 - categorical_accuracy: 0.7588 - val_loss: 0.4448 - val_categorical_accuracy: 0.8908\n",
      "Epoch 3/40\n",
      "60498/60498 [==============================] - 74s 1ms/sample - loss: 0.3967 - categorical_accuracy: 0.8811 - val_loss: 0.2847 - val_categorical_accuracy: 0.9312\n",
      "Epoch 4/40\n",
      "60498/60498 [==============================] - 69s 1ms/sample - loss: 0.2414 - categorical_accuracy: 0.9258 - val_loss: 0.2683 - val_categorical_accuracy: 0.9381\n",
      "Epoch 5/40\n",
      "60498/60498 [==============================] - 70s 1ms/sample - loss: 0.1704 - categorical_accuracy: 0.9478 - val_loss: 0.2025 - val_categorical_accuracy: 0.9457\n",
      "Epoch 6/40\n",
      "60498/60498 [==============================] - 72s 1ms/sample - loss: 0.1303 - categorical_accuracy: 0.9598 - val_loss: 0.1951 - val_categorical_accuracy: 0.9550\n",
      "Epoch 7/40\n",
      "60498/60498 [==============================] - 70s 1ms/sample - loss: 0.1004 - categorical_accuracy: 0.9693 - val_loss: 0.2174 - val_categorical_accuracy: 0.9541\n",
      "Epoch 8/40\n",
      "60498/60498 [==============================] - 65s 1ms/sample - loss: 0.0821 - categorical_accuracy: 0.9747 - val_loss: 0.1974 - val_categorical_accuracy: 0.9580\n",
      "Epoch 9/40\n",
      "60498/60498 [==============================] - 66s 1ms/sample - loss: 0.0713 - categorical_accuracy: 0.9784 - val_loss: 0.1912 - val_categorical_accuracy: 0.9606\n",
      "Epoch 10/40\n",
      "60498/60498 [==============================] - 69s 1ms/sample - loss: 0.0616 - categorical_accuracy: 0.9816 - val_loss: 0.1829 - val_categorical_accuracy: 0.9636\n",
      "Epoch 11/40\n",
      "60498/60498 [==============================] - 69s 1ms/sample - loss: 0.0572 - categorical_accuracy: 0.9828 - val_loss: 0.1942 - val_categorical_accuracy: 0.9626\n",
      "Epoch 12/40\n",
      "60498/60498 [==============================] - 74s 1ms/sample - loss: 0.0485 - categorical_accuracy: 0.9854 - val_loss: 0.1548 - val_categorical_accuracy: 0.9638\n",
      "Epoch 13/40\n",
      "60498/60498 [==============================] - 68s 1ms/sample - loss: 0.0434 - categorical_accuracy: 0.9864 - val_loss: 0.1513 - val_categorical_accuracy: 0.9688\n",
      "Epoch 14/40\n",
      "60498/60498 [==============================] - 64s 1ms/sample - loss: 0.0391 - categorical_accuracy: 0.9886 - val_loss: 0.1785 - val_categorical_accuracy: 0.9648\n",
      "Epoch 15/40\n",
      "60498/60498 [==============================] - 71s 1ms/sample - loss: 0.0396 - categorical_accuracy: 0.9883 - val_loss: 0.1664 - val_categorical_accuracy: 0.9712\n",
      "Epoch 16/40\n",
      "60498/60498 [==============================] - 69s 1ms/sample - loss: 0.0335 - categorical_accuracy: 0.9900 - val_loss: 0.1767 - val_categorical_accuracy: 0.9687\n",
      "Epoch 17/40\n",
      "60498/60498 [==============================] - 76s 1ms/sample - loss: 0.0322 - categorical_accuracy: 0.9907 - val_loss: 0.1905 - val_categorical_accuracy: 0.9658\n"
     ]
    }
   ],
   "source": [
    "#Callback\n",
    "stopper = EarlyStopping('val_loss', 0.005, patience=5)\n",
    "\n",
    "#Arquitectura\n",
    "visible = Input(shape=(100, 100, 3), name='input')\n",
    "conv1 = Conv2D(128, kernel_size=3, activation='relu')(visible)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "conv2 = Conv2D(64, kernel_size=3, activation='relu')(pool1)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "conv3 = Conv2D(32, kernel_size=3, activation='relu')(pool2)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "flat = Flatten()(pool3)\n",
    "hidden1 = Dense(256, activation='relu')(flat)\n",
    "drop1 = Dropout(0.4)(hidden1)\n",
    "hidden2 = Dense(128, activation='relu')(drop1)\n",
    "drop2 = Dropout(0.3)(hidden2)\n",
    "hidden3 = Dense(64, activation='relu')(drop2)\n",
    "output = Dense(120, activation='softmax')(hidden3)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=40,\n",
    "                    callbacks=[stopper],\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como última modificación se aumenta el parámetro del Dropout, logrando resultados incluso mejores en validación llegando al 97% de accuracy en la época 15. Esta arquitectura fue reciclada de un problema previo de clasificación de números de fotografías de boletas, y decidí modificarla agregandole Dropout como en el problema 1, descubriendo que funciona mejor aún. Además, al aumentarle el parámetro del Dropout mejora más aún el score en validación. En definitiva, todas estas mejoras de solo agregar Dropout se traducen en un incremento de cerca de 17% de acurracy en validación, lo que indica un mejor aprendizaje del modelo y menor sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C1jRFYieG-md"
   },
   "source": [
    "### 3.c Batch Normalization\n",
    "Una manera propuesta de mejorar los desempeños de las redes en general, que funciona bastante bien en tareas de reconocimiento de imagenes es _Batch Normalization_. Segun su conocimiento teórico y investigación, ¿Qué realiza _Batch Normalization_ en términos matemáticos? En terminos de aprendizaje, ¿qué evita la utilización de _Batch Normalization_?\n",
    "\n",
    "Entren nuevamente su red preferida de la pregunta anterior, agregando capas de _Batch Normalization_ luego de cada capa de _MaxPool_. Comente sus resultados. \n",
    "\n",
    "¿Mejoran los desempeños de la red agregando _Batch Normalization_? ¿Existe diferencias entre una capa de _batch normalization_ justo antes o justo despues de una capa de _MaxPool_ en términos numéricos? ¿Opina lo mismo en términos de aprendizaje? Discuta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "colab_type": "code",
    "id": "f0KrJDdzDqNI",
    "outputId": "5c1f75e0-cade-4b96-f383-557e96d473d7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60498 samples, validate on 20622 samples\n",
      "Epoch 1/40\n",
      "60498/60498 [==============================] - 78s 1ms/sample - loss: 1.9859 - categorical_accuracy: 0.5221 - val_loss: 0.3914 - val_categorical_accuracy: 0.8981\n",
      "Epoch 2/40\n",
      "60498/60498 [==============================] - 79s 1ms/sample - loss: 0.3685 - categorical_accuracy: 0.8918 - val_loss: 0.1655 - val_categorical_accuracy: 0.9525\n",
      "Epoch 3/40\n",
      "60498/60498 [==============================] - 78s 1ms/sample - loss: 0.1618 - categorical_accuracy: 0.9525 - val_loss: 0.1248 - val_categorical_accuracy: 0.9633\n",
      "Epoch 4/40\n",
      "60498/60498 [==============================] - 80s 1ms/sample - loss: 0.0913 - categorical_accuracy: 0.9726 - val_loss: 0.1082 - val_categorical_accuracy: 0.9695\n",
      "Epoch 5/40\n",
      "60498/60498 [==============================] - 83s 1ms/sample - loss: 0.0613 - categorical_accuracy: 0.9811 - val_loss: 0.0979 - val_categorical_accuracy: 0.9712\n",
      "Epoch 6/40\n",
      "60498/60498 [==============================] - 75s 1ms/sample - loss: 0.0472 - categorical_accuracy: 0.9858 - val_loss: 0.0991 - val_categorical_accuracy: 0.9745\n",
      "Epoch 7/40\n",
      "60498/60498 [==============================] - 73s 1ms/sample - loss: 0.0344 - categorical_accuracy: 0.9899 - val_loss: 0.1150 - val_categorical_accuracy: 0.9686\n",
      "Epoch 8/40\n",
      "60498/60498 [==============================] - 73s 1ms/sample - loss: 0.0309 - categorical_accuracy: 0.9905 - val_loss: 0.0899 - val_categorical_accuracy: 0.9755\n",
      "Epoch 9/40\n",
      "60498/60498 [==============================] - 71s 1ms/sample - loss: 0.0250 - categorical_accuracy: 0.9924 - val_loss: 0.1009 - val_categorical_accuracy: 0.9752\n",
      "Epoch 10/40\n",
      "60498/60498 [==============================] - 72s 1ms/sample - loss: 0.0221 - categorical_accuracy: 0.9929 - val_loss: 0.1180 - val_categorical_accuracy: 0.9724\n",
      "Epoch 11/40\n",
      "60498/60498 [==============================] - 72s 1ms/sample - loss: 0.0204 - categorical_accuracy: 0.9933 - val_loss: 0.1015 - val_categorical_accuracy: 0.9792\n",
      "Epoch 12/40\n",
      "60498/60498 [==============================] - 72s 1ms/sample - loss: 0.0171 - categorical_accuracy: 0.9948 - val_loss: 0.0987 - val_categorical_accuracy: 0.9815\n",
      "Epoch 13/40\n",
      "60498/60498 [==============================] - 72s 1ms/sample - loss: 0.0163 - categorical_accuracy: 0.9948 - val_loss: 0.0904 - val_categorical_accuracy: 0.9794\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "#Callback\n",
    "stopper = EarlyStopping('val_loss', 0.005, patience=5)\n",
    "\n",
    "#Arquitectura\n",
    "visible = Input(shape=(100, 100, 3), name='input')\n",
    "conv1 = Conv2D(128, kernel_size=3, activation='relu')(visible)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "bn1 = BatchNormalization()(pool1)\n",
    "conv2 = Conv2D(64, kernel_size=3, activation='relu')(bn1)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "bn2 = BatchNormalization()(pool2)\n",
    "conv3 = Conv2D(32, kernel_size=3, activation='relu')(bn2)\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "bn3 = BatchNormalization()(pool3)\n",
    "flat = Flatten()(bn3)\n",
    "hidden1 = Dense(256, activation='relu')(flat)\n",
    "drop1 = Dropout(0.4)(hidden1)\n",
    "hidden2 = Dense(128, activation='relu')(drop1)\n",
    "drop2 = Dropout(0.3)(hidden2)\n",
    "hidden3 = Dense(64, activation='relu')(drop2)\n",
    "output = Dense(120, activation='softmax')(hidden3)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=40,\n",
    "                    callbacks=[stopper],\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization es una técnica que busca normalizar las activaciones de una capa respecto de los datos del batch, con el objetivo de fijar una misma distribución para todas las capas. Utilizar Batch Normalization evita el sobreajuste del modelo, ya que la formula de normalización actúa como regularizador. Además, reduce el covariance shift en las capas ocultas y permite usar learning rates más altos (debido a que escala las activaciones), lo que tiene como consecuencia un entrenamiento más rápido.\n",
    "#### Efectivamente, al red mejora su desempeño en validación al agregar Batch normalization, llegando al 98%. Agregar Batch normalization antes o despues de un max pool no afecta si tomamos en cuenta que la capa de maxPool extrae los valores mayores, valores que después de ser escalados mantienen el orden, por lo que el resultado es el mismo. Lo que si cambiaría es la cantidad de datos a escalar, ya que al hacerlo antes del Maxpool se realiza un mayor número de normalizaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tGctPgRG-mf"
   },
   "source": [
    "### 3.d Skip Connections\n",
    "Otra manera de mejorar los resultados de las redes, sobretodo de las redes profundas donde se observa el problema de _vanishing gradient_ son las relativamente nuevas _skip connections_ o redes residuales. En vez de preocuparse de como manejar los pesos de la red para permitir que el gradiente no explote o no desaparezca, se permite al gradiente \"pasar\" sin ser modificado, agregando conecciones con pesos fijos entre capas de distintas profundidades, en la practica permitiendo a la señar \"saltarse\" las capas intermedias. Esta idea ha permitido desarrollos como los de ResNet, llegando a profundidades de cientos de capas y aún logrando aprendizaje. \n",
    "\n",
    "Para implementar estas ideas debemos utilizar la API funcional de Keras al momento de construir los modelos. Algo que debemos notar es que la mayoría de los objetos de keras pueden ser llamados como funciones, y al momento de hacerlo con objetos `layers` es equivalente a conectarlos, por lo cual si hacemos `x(y)` retornamos un objeto con la capa `y` conectada a la capa `x`. \n",
    "\n",
    "Basandose en el código mostrado abajo, implemente una ResNet de su gusto, puede agregar _Skip conections_ a una red utilizada anteriormente o crear una nueva. Con estas redes debería lograr facilmente un _accuracy_ de al menos $90\\%$\n",
    "\n",
    "Comente sus resultados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "colab_type": "code",
    "id": "Vg-rXuqEG-mf",
    "outputId": "3aa13feb-f9e2-43ea-f158-c17237c0b65f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60498 samples, validate on 20622 samples\n",
      "Epoch 1/40\n",
      "60498/60498 [==============================] - 208s 3ms/sample - loss: 1.3137 - categorical_accuracy: 0.6796 - val_loss: 0.2351 - val_categorical_accuracy: 0.9344\n",
      "Epoch 2/40\n",
      "60498/60498 [==============================] - 205s 3ms/sample - loss: 0.1824 - categorical_accuracy: 0.9457 - val_loss: 0.1289 - val_categorical_accuracy: 0.9659\n",
      "Epoch 3/40\n",
      "60498/60498 [==============================] - 204s 3ms/sample - loss: 0.1036 - categorical_accuracy: 0.9680 - val_loss: 0.2005 - val_categorical_accuracy: 0.9640\n",
      "Epoch 4/40\n",
      "60498/60498 [==============================] - 205s 3ms/sample - loss: 0.0705 - categorical_accuracy: 0.9781 - val_loss: 0.0995 - val_categorical_accuracy: 0.9763\n",
      "Epoch 5/40\n",
      "60498/60498 [==============================] - 204s 3ms/sample - loss: 0.0547 - categorical_accuracy: 0.9839 - val_loss: 0.1229 - val_categorical_accuracy: 0.9748\n",
      "Epoch 6/40\n",
      "60498/60498 [==============================] - 204s 3ms/sample - loss: 0.0482 - categorical_accuracy: 0.9854 - val_loss: 0.0871 - val_categorical_accuracy: 0.9845\n",
      "Epoch 7/40\n",
      "60498/60498 [==============================] - 205s 3ms/sample - loss: 0.0376 - categorical_accuracy: 0.9883 - val_loss: 0.0894 - val_categorical_accuracy: 0.9839\n",
      "Epoch 8/40\n",
      "60498/60498 [==============================] - 205s 3ms/sample - loss: 0.0307 - categorical_accuracy: 0.9905 - val_loss: 0.0973 - val_categorical_accuracy: 0.9808\n",
      "Epoch 9/40\n",
      "60498/60498 [==============================] - 204s 3ms/sample - loss: 0.0310 - categorical_accuracy: 0.9904 - val_loss: 0.1170 - val_categorical_accuracy: 0.9813\n",
      "Epoch 10/40\n",
      "60498/60498 [==============================] - 204s 3ms/sample - loss: 0.0250 - categorical_accuracy: 0.9920 - val_loss: 0.1277 - val_categorical_accuracy: 0.9877\n",
      "Epoch 11/40\n",
      "60498/60498 [==============================] - 203s 3ms/sample - loss: 0.0210 - categorical_accuracy: 0.9934 - val_loss: 0.2246 - val_categorical_accuracy: 0.9827\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "#Arquitectura\n",
    "visible = Input(shape=(100, 100, 3), name='input')\n",
    "conv1 = Conv2D(128, kernel_size=3, padding='same', activation='relu')(visible)\n",
    "conv1 = Conv2D(128, kernel_size=3, padding='same', activation='relu')(conv1)\n",
    "\n",
    "res1 = concatenate([conv1, visible])\n",
    "\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(res1)\n",
    "bn1 = BatchNormalization()(pool1)\n",
    "conv2 = Conv2D(64, kernel_size=3, padding='same', activation='relu')(bn1)\n",
    "conv2 = Conv2D(64, kernel_size=3, padding='same', activation='relu')(conv2)\n",
    "\n",
    "res2 = concatenate([conv2, bn1])\n",
    "\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(res2)\n",
    "bn2 = BatchNormalization()(pool2)\n",
    "conv3 = Conv2D(32, kernel_size=3, padding='same', activation='relu')(bn2)\n",
    "conv3 = Conv2D(32, kernel_size=3, padding='same', activation='relu')(conv3)\n",
    "\n",
    "res3 = concatenate([conv3, bn2])\n",
    "\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(res3)\n",
    "bn3 = BatchNormalization()(pool3)\n",
    "flat = Flatten()(bn3)\n",
    "hidden1 = Dense(256, activation='relu')(flat)\n",
    "drop1 = Dropout(0.4)(hidden1)\n",
    "hidden2 = Dense(128, activation='relu')(drop1)\n",
    "drop2 = Dropout(0.3)(hidden2)\n",
    "hidden3 = Dense(64, activation='relu')(drop2)\n",
    "output = Dense(120, activation='softmax')(hidden3)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs=40,\n",
    "                    callbacks=[stopper],\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se agregó 3 veces skip connections al modelo anterior, después de cada convolución. Era díficil pensar en que un modelo que ya tenía 98% de accuracy en validación podría mejorar, pero se ve un leve incremento en el accuracy de todas formas, al igual que la perdida de validación, por lo que podemos asumir que el modelo con skip connections es aún mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SrIOqypNG-mk"
   },
   "source": [
    "### 3.e Red Final \n",
    "\n",
    "De todas las redes entrenadas anteriormente, elija la con mejor desempeño y entrenela utizando aumentación de datos como aprendió en las preguntas anteriores. ¿Mejora el desempeño de la red? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dTPBhMDVjw-c",
    "outputId": "73760333-c4f5-4a9c-8402-d682a1fc3e29",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 13s - loss: 0.6508 - categorical_accuracy: 0.8054\n",
      "1891/1891 - 136s - loss: 2.5098 - categorical_accuracy: 0.4070 - val_loss: 0.7763 - val_categorical_accuracy: 0.8054\n",
      "Epoch 2/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 13s - loss: 0.1667 - categorical_accuracy: 0.9366\n",
      "1891/1891 - 127s - loss: 0.7352 - categorical_accuracy: 0.7869 - val_loss: 0.2393 - val_categorical_accuracy: 0.9366\n",
      "Epoch 3/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.1544 - categorical_accuracy: 0.9558\n",
      "1891/1891 - 126s - loss: 0.3753 - categorical_accuracy: 0.8872 - val_loss: 0.1871 - val_categorical_accuracy: 0.9558\n",
      "Epoch 4/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 13s - loss: 0.0936 - categorical_accuracy: 0.9727\n",
      "1891/1891 - 127s - loss: 0.2418 - categorical_accuracy: 0.9249 - val_loss: 0.1271 - val_categorical_accuracy: 0.9727\n",
      "Epoch 5/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 13s - loss: 0.1825 - categorical_accuracy: 0.9391\n",
      "1891/1891 - 126s - loss: 0.1827 - categorical_accuracy: 0.9430 - val_loss: 0.2683 - val_categorical_accuracy: 0.9391\n",
      "Epoch 6/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 13s - loss: 0.0611 - categorical_accuracy: 0.9833\n",
      "1891/1891 - 126s - loss: 0.1435 - categorical_accuracy: 0.9554 - val_loss: 0.0873 - val_categorical_accuracy: 0.9833\n",
      "Epoch 7/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.0495 - categorical_accuracy: 0.9750\n",
      "1891/1891 - 125s - loss: 0.1165 - categorical_accuracy: 0.9637 - val_loss: 0.0999 - val_categorical_accuracy: 0.9750\n",
      "Epoch 8/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.0483 - categorical_accuracy: 0.9779\n",
      "1891/1891 - 124s - loss: 0.1049 - categorical_accuracy: 0.9676 - val_loss: 0.0990 - val_categorical_accuracy: 0.9779\n",
      "Epoch 9/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.0453 - categorical_accuracy: 0.9834\n",
      "1891/1891 - 124s - loss: 0.0928 - categorical_accuracy: 0.9709 - val_loss: 0.0901 - val_categorical_accuracy: 0.9834\n",
      "Epoch 10/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.0321 - categorical_accuracy: 0.9843\n",
      "1891/1891 - 124s - loss: 0.0807 - categorical_accuracy: 0.9752 - val_loss: 0.0662 - val_categorical_accuracy: 0.9843\n",
      "Epoch 11/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.0874 - categorical_accuracy: 0.9785\n",
      "1891/1891 - 124s - loss: 0.0737 - categorical_accuracy: 0.9776 - val_loss: 0.1834 - val_categorical_accuracy: 0.9785\n",
      "Epoch 12/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.1048 - categorical_accuracy: 0.9644\n",
      "1891/1891 - 124s - loss: 0.0701 - categorical_accuracy: 0.9787 - val_loss: 0.2089 - val_categorical_accuracy: 0.9644\n",
      "Epoch 13/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.0559 - categorical_accuracy: 0.9860\n",
      "1891/1891 - 124s - loss: 0.0650 - categorical_accuracy: 0.9810 - val_loss: 0.1127 - val_categorical_accuracy: 0.9860\n",
      "Epoch 14/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.2585 - categorical_accuracy: 0.9470\n",
      "1891/1891 - 124s - loss: 0.0531 - categorical_accuracy: 0.9834 - val_loss: 0.3204 - val_categorical_accuracy: 0.9470\n",
      "Epoch 15/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.0645 - categorical_accuracy: 0.9802\n",
      "1891/1891 - 124s - loss: 0.0552 - categorical_accuracy: 0.9841 - val_loss: 0.1354 - val_categorical_accuracy: 0.9802\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Dropout, concatenate, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    brightness_range=None,\n",
    "    shear_range=0.0,\n",
    "    zoom_range=0.1,\n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='constant',\n",
    "    cval=0.0,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    rescale=None)\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "stopper = EarlyStopping('val_loss', 0.005, patience=5)\n",
    "\n",
    "visible = Input(shape=(100, 100, 3), name='input')\n",
    "conv1 = Conv2D(128, kernel_size=3, padding='same', activation='relu')(visible)\n",
    "conv1 = Conv2D(128, kernel_size=3, padding='same', activation='relu')(conv1)\n",
    "\n",
    "res1 = concatenate([conv1, visible])\n",
    "\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(res1)\n",
    "bn1 = BatchNormalization()(pool1)\n",
    "conv2 = Conv2D(64, kernel_size=3, padding='same', activation='relu')(bn1)\n",
    "conv2 = Conv2D(64, kernel_size=3, padding='same', activation='relu')(conv2)\n",
    "\n",
    "res2 = concatenate([conv2, bn1])\n",
    "\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(res2)\n",
    "bn2 = BatchNormalization()(pool2)\n",
    "conv3 = Conv2D(32, kernel_size=3, padding='same', activation='relu')(bn2)\n",
    "conv3 = Conv2D(32, kernel_size=3, padding='same', activation='relu')(conv3)\n",
    "\n",
    "res3 = concatenate([conv3, bn2])\n",
    "\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(res3)\n",
    "bn3 = BatchNormalization()(pool3)\n",
    "flat = Flatten()(bn3)\n",
    "hidden1 = Dense(256, activation='relu')(flat)\n",
    "drop1 = Dropout(0.4)(hidden1)\n",
    "hidden2 = Dense(128, activation='relu')(drop1)\n",
    "drop2 = Dropout(0.3)(hidden2)\n",
    "hidden3 = Dense(64, activation='relu')(drop2)\n",
    "output = Dense(120, activation='softmax')(hidden3)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "history = model.fit_generator(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                    epochs=50,\n",
    "                    verbose=2,\n",
    "                    callbacks=[stopper],\n",
    "                    validation_data=(X_val,y_val),\n",
    "                    validation_freq=1,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A estas altura ya cuesta diferenciar si las diferencias en los resultados son por mejoras en el modelo o producto del random. De cualquier forma, los resultados se ven similares al anterior. Creo que es curioso notar que, las imagenes del conjunto de prueba fueron tomadas con una cámara de video mientras eran rotadas, por lo que las imágenes ya poseen un cierto grado de aumentación en sí mismas, lo que puede haber minimizado el efecto de nuestra aumentación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_cqspYE0G-ml"
   },
   "source": [
    "### 3.f Modelo para frutas\n",
    "Repita la el item anterior, pero esta vez sobre el problema de clasificación de las frutas y verduras independiente de sus variedades. Para esto deberá cargar nuevamente los datos, esta vez transformando las categorías que originalmente eran un `string` a la primera palabra del string. Así al momento de hacer el paso a categorías y _one hot vector_, todas las imagenes con la misma fruta o verdura quedarán en la misma categoría. \n",
    "\n",
    "- ¿Cómo se desempeña el modelo esta vez? \n",
    "- ¿_A priori_ cuál habría considerado el problema más dificil para un modelo de aprendizaje de máquinas? Discuta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "nkGIPOqSG-mm",
    "outputId": "923d38ba-12cb-4dd2-f168-983e86fbff06",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Epoch 1/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.3835 - categorical_accuracy: 0.8569\n",
      "1891/1891 - 137s - loss: 1.9122 - categorical_accuracy: 0.5082 - val_loss: 0.5331 - val_categorical_accuracy: 0.8569\n",
      "Epoch 2/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.2824 - categorical_accuracy: 0.9186\n",
      "1891/1891 - 130s - loss: 0.5597 - categorical_accuracy: 0.8357 - val_loss: 0.3391 - val_categorical_accuracy: 0.9186\n",
      "Epoch 3/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.4281 - categorical_accuracy: 0.9590\n",
      "1891/1891 - 131s - loss: 0.2860 - categorical_accuracy: 0.9154 - val_loss: 0.6247 - val_categorical_accuracy: 0.9590\n",
      "Epoch 4/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.1700 - categorical_accuracy: 0.9736\n",
      "1891/1891 - 130s - loss: 0.1977 - categorical_accuracy: 0.9400 - val_loss: 0.2912 - val_categorical_accuracy: 0.9736\n",
      "Epoch 5/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.1098 - categorical_accuracy: 0.9700\n",
      "1891/1891 - 129s - loss: 0.1380 - categorical_accuracy: 0.9585 - val_loss: 0.2209 - val_categorical_accuracy: 0.9700\n",
      "Epoch 6/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.0797 - categorical_accuracy: 0.9745\n",
      "1891/1891 - 128s - loss: 0.1154 - categorical_accuracy: 0.9649 - val_loss: 0.1656 - val_categorical_accuracy: 0.9745\n",
      "Epoch 7/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.0737 - categorical_accuracy: 0.9649\n",
      "1891/1891 - 128s - loss: 0.0950 - categorical_accuracy: 0.9717 - val_loss: 0.1447 - val_categorical_accuracy: 0.9649\n",
      "Epoch 8/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.0955 - categorical_accuracy: 0.9708\n",
      "1891/1891 - 126s - loss: 0.0832 - categorical_accuracy: 0.9755 - val_loss: 0.1648 - val_categorical_accuracy: 0.9708\n",
      "Epoch 9/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.1729 - categorical_accuracy: 0.9348\n",
      "1891/1891 - 126s - loss: 0.0704 - categorical_accuracy: 0.9797 - val_loss: 0.3185 - val_categorical_accuracy: 0.9348\n",
      "Epoch 10/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.0181 - categorical_accuracy: 0.9906\n",
      "1891/1891 - 125s - loss: 0.0699 - categorical_accuracy: 0.9800 - val_loss: 0.0367 - val_categorical_accuracy: 0.9906\n",
      "Epoch 11/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.0437 - categorical_accuracy: 0.9763\n",
      "1891/1891 - 126s - loss: 0.0568 - categorical_accuracy: 0.9830 - val_loss: 0.0823 - val_categorical_accuracy: 0.9763\n",
      "Epoch 12/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.0200 - categorical_accuracy: 0.9881\n",
      "1891/1891 - 126s - loss: 0.0598 - categorical_accuracy: 0.9830 - val_loss: 0.0407 - val_categorical_accuracy: 0.9881\n",
      "Epoch 13/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.1036 - categorical_accuracy: 0.9586\n",
      "1891/1891 - 126s - loss: 0.0482 - categorical_accuracy: 0.9865 - val_loss: 0.2149 - val_categorical_accuracy: 0.9586\n",
      "Epoch 14/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.0262 - categorical_accuracy: 0.9851\n",
      "1891/1891 - 125s - loss: 0.0483 - categorical_accuracy: 0.9867 - val_loss: 0.0544 - val_categorical_accuracy: 0.9851\n",
      "Epoch 15/50\n",
      "Epoch 1/50\n",
      "20622/1891 - 12s - loss: 0.1436 - categorical_accuracy: 0.9917\n",
      "1891/1891 - 125s - loss: 0.0438 - categorical_accuracy: 0.9875 - val_loss: 0.0476 - val_categorical_accuracy: 0.9917\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Dropout, concatenate, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=0,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    brightness_range=None,\n",
    "    shear_range=0.0,\n",
    "    zoom_range=0.1,\n",
    "    channel_shift_range=0.0,\n",
    "    fill_mode='constant',\n",
    "    cval=0.0,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,\n",
    "    rescale=None)\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "stopper = EarlyStopping('val_loss', 0.005, patience=5)\n",
    "\n",
    "visible = Input(shape=(100, 100, 3), name='input')\n",
    "conv1 = Conv2D(128, kernel_size=3, padding='same', activation='relu')(visible)\n",
    "conv1 = Conv2D(128, kernel_size=3, padding='same', activation='relu')(conv1)\n",
    "\n",
    "res1 = concatenate([conv1, visible])\n",
    "\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(res1)\n",
    "bn1 = BatchNormalization()(pool1)\n",
    "conv2 = Conv2D(64, kernel_size=3, padding='same', activation='relu')(bn1)\n",
    "conv2 = Conv2D(64, kernel_size=3, padding='same', activation='relu')(conv2)\n",
    "\n",
    "res2 = concatenate([conv2, bn1])\n",
    "\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(res2)\n",
    "bn2 = BatchNormalization()(pool2)\n",
    "conv3 = Conv2D(32, kernel_size=3, padding='same', activation='relu')(bn2)\n",
    "conv3 = Conv2D(32, kernel_size=3, padding='same', activation='relu')(conv3)\n",
    "\n",
    "res3 = concatenate([conv3, bn2])\n",
    "\n",
    "pool3 = MaxPooling2D(pool_size=(2, 2))(res3)\n",
    "bn3 = BatchNormalization()(pool3)\n",
    "flat = Flatten()(bn3)\n",
    "hidden1 = Dense(256, activation='relu')(flat)\n",
    "drop1 = Dropout(0.4)(hidden1)\n",
    "hidden2 = Dense(128, activation='relu')(drop1)\n",
    "drop2 = Dropout(0.3)(hidden2)\n",
    "hidden3 = Dense(64, activation='relu')(drop2)\n",
    "output = Dense(63, activation='softmax')(hidden3)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "adam = Adam(lr=0.0001)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "history = model.fit_generator(datagen.flow(X_train, y_train2, batch_size=32),\n",
    "                    epochs=50,\n",
    "                    verbose=2,\n",
    "                    callbacks=[stopper],\n",
    "                    validation_data=(X_val,y_val2),\n",
    "                    validation_freq=1,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El rendimiento para este caso es mejor que para el caso anterior, aunque la mejora sea menor a 1% de accuracy en validación.\n",
    "#### Para mí, la dificultad de un problema de clasificación suele darse por factores como clases mal balanceadas, alto número de clases, clases muy parecidas entre ellas y pocos datos, entre otros. A priori, habría considerado como problema más dificil el problema anterior, ya que tiene clases muy similares unas con otras (por ejemplo, variedades de manzanas). Es en estos casos donde el extractor de características debe ser muy bueno para poder diferenciar estas clases parecidas. Además, se reduce el número de clases a clasificar, lo que debería significar que el modelo pueda obtener probabilidades más certeras para cada clase. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "RYOwa3NMG-kw",
    "tdrih9MOG-k1",
    "-9Z2j4uiG-k8",
    "x3-T18tkG-lC",
    "72-JP1ieG-lH",
    "bHdXHEiXG-lN",
    "41SY1xlUG-ld",
    "7vAU5ESqG-lk",
    "m41rBXEWG-ls",
    "41x3sGUlG-lw",
    "ykxYDM3TG-lz",
    "2_v7x0tVG-l4",
    "vrkhv6tvG-l8",
    "56vV2peEG-mA",
    "SQwUxopqG-mE",
    "AGRqfQ6uG-mJ",
    "BEh44T_dG-mL",
    "VNsonS6_G-mR",
    "_DTu0O8PG-mV",
    "3Is-TnidG-md",
    "C1jRFYieG-md",
    "6tGctPgRG-mf",
    "SrIOqypNG-mk",
    "_cqspYE0G-ml"
   ],
   "machine_shape": "hm",
   "name": "Pregunta 3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
