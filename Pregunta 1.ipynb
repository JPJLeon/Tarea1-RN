{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1dvfXS9G-jF"
   },
   "source": [
    "### <img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-395/477 Redes Neuronales Artificiales - 2019-2 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 1  </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "* Manipulaciones en pandas y numpy, preprocesamientos\n",
    "* Redes Densas Feed Forward\n",
    "* Regularización y Dropout\n",
    "* Vanishing Gradient y Skip Connections\n",
    "* Learn Rate Decay\n",
    "* Optimizadores\n",
    "* Redes Convolucionales\n",
    "* Image Data Agumentation\n",
    "\n",
    "\n",
    "**Formalidades**  \n",
    "* Equipos de trabajo de 2 personas (*Ambos estudiantes deben estar preparados para presentar la tarea el día de la entrega*)\n",
    "* El entregable debe ser un _Jupyter Notebook_ incluyendo los códigos utilizados, los resultados, los gráficos realizados y comentarios. Debe seguir una estructura similar a un informe (se debe introducir los problemas a trabajar, presentar los resultados y discutirlos), se penalizará fuertemente ausencia de comentarios, explicaciones de gráficos, _etc_. Si lo prefiere puede entregar un _Jupyter Notebook_ por pregunta o uno por toda la tarea, con tal de que todos los entregables esten bien identificados y se encuentren en el mismo repositorio de _Github_.\n",
    "* Se debe preparar una presentación del trabajo realizado y sus hallazgos. El presentador será elegido aleatoriamente y deberá apoyarse en el _Jupyter Notebook_ que entregarán. \n",
    "* Formato de entrega: envı́o de link del repositorio en _Github_, al correo electrónico del ayudante (<alvaro.valderrama.13@sansano.usm.cl>), en copia al profesor (<cvalle@inf.utfsm.cl>).   Especificar el siguiente asunto: [INF-395/477-2019 Tarea 1]. Invitar como colaborador al usuario de github \"avalderr\" para poder acceder al repositorio en caso de ser privado.\n",
    "* Fecha de entrega y presentaciones: 22 de Noviembre. Hora límite de entrega: 23:59. Cualquier _commit_ luego de la hora límite no será evaluado. Se realizará descuento por atrasos en envío del mail igualmente.  \n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "La tarea se divide en tres partes:\n",
    "\n",
    "* 1 - Redes Feed Forward para Airbnb\n",
    "* 2 - Reconocimiento de Imagenes en CIFAR10    \n",
    "* 3 - Reconocimiento de frutas y verduras\n",
    "\n",
    "La tarea tiene ejemplos de códigos con los cuales pueden guiarse en gran parte, sin embargo, solo son guias y pueden ser creativos al momento de resolver la tarea. Soluciones creativas o elegantes serán valoradas. También en algunas ocaciones se hacen elecciones arbitrarias, ustedes pueden realizar otras elecciones con tal de que haya una pequeña justificación de por qué su elección es mejor o equivalente.\n",
    "Recuerden intercalar su código con *comentarios* en celdas _Markdown_, con los comentarios de la pregunta y con cualquier analisis, fórmula (en $ \\LaTeX $) o explicación que les parezca relevante para justificar sus procedimientos. *No respondan las preguntas en comentarios en el código*.\n",
    "Noten que en general cuando se les pide elegir algo o proponer algo no se evaluará tanto la elección en si. En cambio la argumentación detrás de la elección será lo más ponderado.\n",
    "Si algun modelo se demora demasiado en correr en su maquina, no olvide que puede correr _Jupyter Notebooks_ en _Collab_ de Google, incluso con la opción de aceleración con GPU (particularmente útil para los modelos más grandes), esto puede ser relevante para las maquinas más lentas al momento de realizar exploraciones con _K-folds_ o las redes más grandes. Existe también la posibilidad de utilizar _Google Cloud Plataform_, donde tienen 300 dolares de prueba por un año y pueden comprar tiempo de procesamiento en maquinas aceleradas con GPU; maquinas ya configuradas para _deep leraning_ pueden encontrarse en el _Marketplace_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZhRK47TDG-jK"
   },
   "source": [
    "_________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0u9WalVG-jN"
   },
   "source": [
    "# 1 - Redes Feed Forward para Airbnb\n",
    "\n",
    "De las redes neuronales artificiales más simples se encuentran las redes densas o _Feed Forward_, donde todas las neuronas de una capa estan conectadas a todos los inputs y envian su señal de activación a todas las neuronas de la siguiente capa. Estas redes, si bien son las más simples, suelen tener desempeños bastante buenos, y en muchas aplicaciones reales son utilizadas, ya sea por si solas o en combinación con otros modelos. Además, son las redes donde más facil se pueden observar muchos de los fenómenos que se han descubierto a lo largo de los años de desarrollo de esta area del conocimiento, tanto por ser de las redes vigentes más antiguas y por su estructura relativamente simple. En esta primera parte de la tarea exploraremos las redes densas y algunos de sus hiperparámetros más relevantes como la profundidad, el número de unidades; estudiaremos también algunos métodos de regularización y evidenciaremos el problema del _vanishing gradient_ y el _exploding gradient_, viendo también algunos optimizadores existentes. \n",
    "\n",
    "Para realizar esto, utilizaremos una base de datos de precios de inmuebles anunciados en Airbnb, la cual se encuentra disponible en _Kaggle_, en la siguiente URL: https://www.kaggle.com/stevezhenghp/airbnb-price-prediction. El dataset cuenta de casi mil registros, donde podemos encontrar el logaritmo del precio del anuncio, el tipo de propiedad, las amenidades disponibles, el número de personas que puede alojar, el número de baños, entre otros. \n",
    "Nuesta tarea durante esta pregunta será predecir el valor del logaritmo del precio del anuncio a partir de algunas de las otras variables presentes en el dataset. Para esto primero deberán preprocesar los datos para transformarlos a una forma que pueda ser utilizada por una red neuronal o eliminarlos en el caso que se estime conveniente. Una vez separados los datos de entrenamiento, validación y test, procederemos a entrenar diferentes modelos, comparandolos y evaluando sus desempeños. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vrp-fhH2G-jQ"
   },
   "source": [
    "### 1.a Carga de datos y primeros analisis\n",
    "Cargue los datos en un _dataframe_ como muestra el código. Explore superficialmente los datos utilizando los metodos `.head`, `.describe` o `.info` del _DataFrame_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "Puw8ujKfJq_k",
    "outputId": "acc36871-9d02-4eb2-b5b6-65e326186ea6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>log_price</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>review_scores_rating</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.411100e+04</td>\n",
       "      <td>74111.000000</td>\n",
       "      <td>74111.000000</td>\n",
       "      <td>73911.000000</td>\n",
       "      <td>74111.000000</td>\n",
       "      <td>74111.000000</td>\n",
       "      <td>74111.000000</td>\n",
       "      <td>57389.000000</td>\n",
       "      <td>74020.000000</td>\n",
       "      <td>73980.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.126662e+07</td>\n",
       "      <td>4.782069</td>\n",
       "      <td>3.155146</td>\n",
       "      <td>1.235263</td>\n",
       "      <td>38.445958</td>\n",
       "      <td>-92.397525</td>\n",
       "      <td>20.900568</td>\n",
       "      <td>94.067365</td>\n",
       "      <td>1.265793</td>\n",
       "      <td>1.710868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.081735e+06</td>\n",
       "      <td>0.717394</td>\n",
       "      <td>2.153589</td>\n",
       "      <td>0.582044</td>\n",
       "      <td>3.080167</td>\n",
       "      <td>21.705322</td>\n",
       "      <td>37.828641</td>\n",
       "      <td>7.836556</td>\n",
       "      <td>0.852143</td>\n",
       "      <td>1.254142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.440000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.338905</td>\n",
       "      <td>-122.511500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.261964e+06</td>\n",
       "      <td>4.317488</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>34.127908</td>\n",
       "      <td>-118.342374</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.225415e+07</td>\n",
       "      <td>4.709530</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.662138</td>\n",
       "      <td>-76.996965</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.640226e+07</td>\n",
       "      <td>5.220356</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.746096</td>\n",
       "      <td>-73.954660</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.123090e+07</td>\n",
       "      <td>7.600402</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>42.390437</td>\n",
       "      <td>-70.985047</td>\n",
       "      <td>605.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     log_price  accommodates     bathrooms      latitude  \\\n",
       "count  7.411100e+04  74111.000000  74111.000000  73911.000000  74111.000000   \n",
       "mean   1.126662e+07      4.782069      3.155146      1.235263     38.445958   \n",
       "std    6.081735e+06      0.717394      2.153589      0.582044      3.080167   \n",
       "min    3.440000e+02      0.000000      1.000000      0.000000     33.338905   \n",
       "25%    6.261964e+06      4.317488      2.000000      1.000000     34.127908   \n",
       "50%    1.225415e+07      4.709530      2.000000      1.000000     40.662138   \n",
       "75%    1.640226e+07      5.220356      4.000000      1.000000     40.746096   \n",
       "max    2.123090e+07      7.600402     16.000000      8.000000     42.390437   \n",
       "\n",
       "          longitude  number_of_reviews  review_scores_rating      bedrooms  \\\n",
       "count  74111.000000       74111.000000          57389.000000  74020.000000   \n",
       "mean     -92.397525          20.900568             94.067365      1.265793   \n",
       "std       21.705322          37.828641              7.836556      0.852143   \n",
       "min     -122.511500           0.000000             20.000000      0.000000   \n",
       "25%     -118.342374           1.000000             92.000000      1.000000   \n",
       "50%      -76.996965           6.000000             96.000000      1.000000   \n",
       "75%      -73.954660          23.000000            100.000000      1.000000   \n",
       "max      -70.985047         605.000000            100.000000     10.000000   \n",
       "\n",
       "               beds  \n",
       "count  73980.000000  \n",
       "mean       1.710868  \n",
       "std        1.254142  \n",
       "min        0.000000  \n",
       "25%        1.000000  \n",
       "50%        1.000000  \n",
       "75%        2.000000  \n",
       "max       18.000000  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#df_full = pd.read_csv(io.BytesIO(uploaded['train.csv']))\n",
    "df_full = pd.read_csv(\"train.csv\")\n",
    "\n",
    "df_full.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5LxvRhRG-jc"
   },
   "source": [
    "Separaremos las columnas en distintas categorías pues deberan ser tratadas de maneras distintas. Las columnas \"others\" y \"categorical\" ya están separadas, complete las numéricas y las fechas.\n",
    "\n",
    "¿Qué particularidad tiene las columnas agrupadas en \"otros\" y porque esto nos complicará su utilización?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "1AHGVma5G-jf",
    "outputId": "8c7cb7be-0d2e-45a7-df43-7e99e670f564"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amenities\n",
      "description\n",
      "thumbnail_url\n",
      "name\n"
     ]
    }
   ],
   "source": [
    "other_col = df_full.columns[[4, 11, 25, 21]]\n",
    "cat_col = df_full.columns[[0, 2, 3, 7, 8, 9, 10, 13, 14, 17, 22, 26]]\n",
    "num_col = df_full.columns[[1, 5, 6, 15, 19, 20, 23, 24, 27, 28]]\n",
    "date_col = df_full.columns[[12, 16, 18]]\n",
    "\n",
    "for col in df_full[other_col].columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BVIgNmz0G-jl"
   },
   "source": [
    "Explore más en detalle la columna `amenities` y explique por qué sería interesante rescatar la información contenida en ella tomando en cuenta el problema en cuestión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "7qHBWS-nG-jn",
    "outputId": "a0d2e3e9-f533-40f0-8a7a-957d392387bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {\"Wireless Internet\",\"Air conditioning\",Kitche...\n",
       "1    {\"Wireless Internet\",\"Air conditioning\",Kitche...\n",
       "2    {TV,\"Cable TV\",\"Wireless Internet\",\"Air condit...\n",
       "3    {TV,\"Cable TV\",Internet,\"Wireless Internet\",Ki...\n",
       "4    {TV,Internet,\"Wireless Internet\",\"Air conditio...\n",
       "Name: amenities, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full['amenities'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-c6TXmwWWUIA"
   },
   "source": [
    "Son variables categoricas que estan representadas como texto. Tiene sentido incluirlas porque entre más comodidades tenga un departamento, es lógico pensar que su valor debería ser mayor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKss1lirG-js"
   },
   "source": [
    "### 1.b Amenities como categórica\n",
    "En esta pregunta extraeremos cada una de las `amenities` posibles y la representaremos como una columna categorica, es decir una columna con un 1 si la esa característica aparece como disponible en el anuncio y un 0 en caso contrario. \n",
    "Para esto primero extraiga un conjunto de todas las amenities posibles. Puede utilizar el método `.apply` de las `Series` de pandas para transformar las entradas de la columna a una lista de las _amenities_ como muestra el código. Luego puede usar un `set` para evitar repeticiones inecesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "0wbLsN7GG-jv",
    "outputId": "1b37935e-4648-4f4a-8ed8-27b8e06c1eae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'', 'Elevator', 'Hot water kettle', 'High chair', 'Dishwasher', 'Keypad', 'Disabled parking spot', 'Internet', 'Gym', 'First aid kit', 'Luggage dropoff allowed', 'Private living room', 'Other', 'Pets allowed', 'Toilet paper', 'Breakfast', 'Kitchen', 'Bathtub with shower chair', 'Accessible-height toilet', 'Lake access', 'Iron', 'Window guards', 'Beach essentials', 'Pack ’n Play/travel crib', 'Private bathroom', 'Hangers', 'Accessible-height bed', 'Buzzer/wireless intercom', 'Smoking allowed', 'Self Check-In', 'Shampoo', 'Flat smooth pathway to front door', 'Lock on bedroom door', 'Air conditioning', 'Indoor fireplace', 'Roll-in shower with chair', 'Children’s dinnerware', 'Hot tub', 'Cable TV', 'Fire extinguisher', 'Lockbox', 'Suitable for events', 'Microwave', 'Ethernet connection', 'translation missing: en.hosting_amenity_50', 'Doorman Entry', 'Private entrance', 'Wide entryway', 'Heating', 'Wheelchair accessible', 'Wide hallway clearance', 'Free parking on premises', ' smooth pathway to front door', 'Firm mattress', 'Carbon monoxide detector', 'Family/kid friendly', 'Pool', 'Body soap', 'Children’s books and toys', 'Smartlock', 'Baby monitor', 'Safety card', 'Dryer', 'Extra pillows and blankets', 'Fireplace guards', 'Crib', 'Washer / Dryer', 'Room-darkening shades', 'EV charger', 'Handheld shower head', 'Babysitter recommendations', 'Doorman', 'Wireless Internet', 'Host greets you', 'Table corner guards', 'Ground floor access', 'Game console', 'Essentials', 'Smart lock', 'Wide clearance to shower and toilet', 'Garden or backyard', 'Dishes and silverware', 'Patio or balcony', 'Ski in/Ski out', 'Well-lit path to entrance', 'Oven', 'Refrigerator', 'Wide clearance to bed', 'Other pet(s)', 'Changing table', 'Coffee maker', 'Elevator in building', 'Grab-rails for shower and toilet', 'Outlet covers', '24-hour check-in', 'Step-free access', 'Stair gates', 'Bath towel', 'Pets live on this property', 'translation missing: en.hosting_amenity_49', 'Long term stays allowed', 'Waterfront', 'Beachfront', 'Cooking basics', 'Stove', 'Wide clearance to shower & toilet', 'Smoke detector', 'Flat', 'Hair dryer', 'Bathtub', 'Air purifier', 'Baby bath', 'Dog(s)', 'Free parking on street', 'Firm matress', 'Fixed grab bars for shower & toilet', 'Laptop friendly workspace', 'Path to entrance lit at night', 'TV', 'Single level home', 'Pocket wifi', 'Hot water', 'Washer', 'Hand soap', 'Paid parking off premises', 'Hand or paper towel', 'Cleaning before checkout', 'BBQ grill', 'Cat(s)', 'Bed linens', 'Wide doorway'}\n",
      "Hay un total de 130 amenities posibles\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "amenities_as_lists = df_full['amenities'].apply(lambda x: np.array(x.replace(\"{\",'').replace(\"}\",'').replace(\"\\\"\",'').split(',')))\n",
    "\n",
    "amenities_set = set(np.hstack(amenities_as_lists.values))\n",
    "print(amenities_set)\n",
    "\n",
    "print(\"Hay un total de\", len(amenities_set) - 1, \"amenities posibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUf0D4fVG-j2"
   },
   "source": [
    "Cuente la cantidad de apariciones de cada _amenity_ en el _dataset_. \n",
    "\n",
    "Claramente algunos valores tienen muy pocos ejemplos, lo cual tiene sentido considerando el problema. Optaremos por no considerar las amenities que aparezcan en menos de 1000 anuncios. \n",
    "\n",
    "Cree ahora nuevas columnas en el _DataFrame_, donde cada columna corresponda a una _amenity_ que cumpla el criterio y se represente binariamente, como muestra el esqueleto de código.\n",
    "\n",
    "Aprovecharemos de eliminar las otras columas `others` pues para el alcance de esta tarea no vale la pena considerarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "id": "1FPq8CB1G-j4",
    "outputId": "423b7ce6-c468-402b-9379-3e8920cbe8dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wireless Internet                      71265\n",
      "Kitchen                                67526\n",
      "Heating                                67073\n",
      "Essentials                             64005\n",
      "Smoke detector                         61727\n",
      "                                       ...  \n",
      "Hand soap                                  1\n",
      "Bath towel                                 1\n",
      "Wide clearance to shower and toilet        1\n",
      "Body soap                                  1\n",
      "Hand or paper towel                        1\n",
      "Length: 131, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>log_price</th>\n",
       "      <th>property_type</th>\n",
       "      <th>room_type</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bed_type</th>\n",
       "      <th>cancellation_policy</th>\n",
       "      <th>cleaning_fee</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>Has_Hair dryer</th>\n",
       "      <th>Has_Bathtub</th>\n",
       "      <th>Has_Dog(s)</th>\n",
       "      <th>Has_Laptop friendly workspace</th>\n",
       "      <th>Has_TV</th>\n",
       "      <th>Has_Hot water</th>\n",
       "      <th>Has_Washer</th>\n",
       "      <th>Has_Cat(s)</th>\n",
       "      <th>Has_Bed linens</th>\n",
       "      <th>Has_Wide doorway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6901257</td>\n",
       "      <td>5.010635</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>strict</td>\n",
       "      <td>True</td>\n",
       "      <td>NYC</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6304928</td>\n",
       "      <td>5.129899</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>strict</td>\n",
       "      <td>True</td>\n",
       "      <td>NYC</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7919400</td>\n",
       "      <td>4.976734</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>moderate</td>\n",
       "      <td>True</td>\n",
       "      <td>NYC</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13418779</td>\n",
       "      <td>6.620073</td>\n",
       "      <td>House</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>flexible</td>\n",
       "      <td>True</td>\n",
       "      <td>SF</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3808709</td>\n",
       "      <td>4.744932</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>moderate</td>\n",
       "      <td>True</td>\n",
       "      <td>DC</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  log_price property_type        room_type  accommodates  \\\n",
       "0   6901257   5.010635     Apartment  Entire home/apt             3   \n",
       "1   6304928   5.129899     Apartment  Entire home/apt             7   \n",
       "2   7919400   4.976734     Apartment  Entire home/apt             5   \n",
       "3  13418779   6.620073         House  Entire home/apt             4   \n",
       "4   3808709   4.744932     Apartment  Entire home/apt             2   \n",
       "\n",
       "   bathrooms  bed_type cancellation_policy  cleaning_fee city  ...  \\\n",
       "0        1.0  Real Bed              strict          True  NYC  ...   \n",
       "1        1.0  Real Bed              strict          True  NYC  ...   \n",
       "2        1.0  Real Bed            moderate          True  NYC  ...   \n",
       "3        1.0  Real Bed            flexible          True   SF  ...   \n",
       "4        1.0  Real Bed            moderate          True   DC  ...   \n",
       "\n",
       "  Has_Hair dryer Has_Bathtub Has_Dog(s) Has_Laptop friendly workspace Has_TV  \\\n",
       "0              1           0          0                             0      0   \n",
       "1              1           0          0                             0      0   \n",
       "2              1           0          0                             1      1   \n",
       "3              0           0          0                             0      1   \n",
       "4              0           0          0                             0      1   \n",
       "\n",
       "  Has_Hot water Has_Washer  Has_Cat(s)  Has_Bed linens Has_Wide doorway  \n",
       "0             0          0           0               0                0  \n",
       "1             0          1           0               0                0  \n",
       "2             0          0           0               0                0  \n",
       "3             0          1           0               0                0  \n",
       "4             0          0           0               0                0  \n",
       "\n",
       "[5 rows x 94 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amenities_splitted = pd.Series(np.hstack(amenities_as_lists.values))\n",
    "print(amenities_splitted.value_counts())\n",
    "\n",
    "allowed_amenities = [j for j, i in amenities_splitted.value_counts().iteritems() if i > 1000]\n",
    "\n",
    "for amenity in amenities_set:\n",
    "    if amenity in allowed_amenities:\n",
    "        df_full['Has_'+amenity] = df_full['amenities'].apply(lambda x: 1 if amenity in x else 0)\n",
    "\n",
    "df = df_full.drop(columns=other_col)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7nvKYD_DG-j-"
   },
   "source": [
    "### 1.c Otras variables categoricas\n",
    "Para cada una de las variables categoricas, cuente cuantos valores únicos tiene en el dataset. ¿Cuales la llaman la atención y por qué? ¿Tiene esto sentido con la naturaleza del problema?\n",
    "\n",
    "Eliminaremos las variables `id` pues solo sirve para identificar cada anuncio y la variable `zipcode` pues representa una información similar a la de `neighbourhood` y en la realidad los usuarios se interesan más por la segunda que por la primera. \n",
    "\n",
    "Cuente cuantos valores tiene cada clase de algunas variables categóricas que le interesen, usando el metodo `.value_counts` de las `Series`.\n",
    "\n",
    "Contaremos también los valores NA (valores ausentes o corrompidos) en todo el dataset con el código dentro del `print`. Eliminaremos todos aquellas variables que tengan más de un 10% de valores ausentes, es decir las 4 columnas presentes en el código. Para evitar problemas más adelante, complete el resto de los valores ausentes con el valor 0, usando el metodo `.fillna`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qMHGZaCEG-j_",
    "outputId": "2674b146-76d9-4178-cc96-b49cd2676e18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apartment             49003\n",
      "House                 16511\n",
      "Condominium            2658\n",
      "Townhouse              1692\n",
      "Loft                   1244\n",
      "Other                   607\n",
      "Guesthouse              498\n",
      "Bed & Breakfast         462\n",
      "Bungalow                366\n",
      "Villa                   179\n",
      "Dorm                    142\n",
      "Guest suite             123\n",
      "Camper/RV                94\n",
      "Timeshare                77\n",
      "Cabin                    72\n",
      "In-law                   71\n",
      "Hostel                   70\n",
      "Boutique hotel           69\n",
      "Boat                     65\n",
      "Serviced apartment       21\n",
      "Tent                     18\n",
      "Castle                   13\n",
      "Vacation home            11\n",
      "Yurt                      9\n",
      "Hut                       8\n",
      "Treehouse                 7\n",
      "Chalet                    6\n",
      "Earth House               4\n",
      "Tipi                      3\n",
      "Cave                      2\n",
      "Train                     2\n",
      "Island                    1\n",
      "Lighthouse                1\n",
      "Casa particular           1\n",
      "Parking Space             1\n",
      "Name: property_type, dtype: int64\n",
      "Entire home/apt    41310\n",
      "Private room       30638\n",
      "Shared room         2163\n",
      "Name: room_type, dtype: int64\n",
      "Real Bed         72028\n",
      "Futon              753\n",
      "Pull-out Sofa      585\n",
      "Airbed             477\n",
      "Couch              268\n",
      "Name: bed_type, dtype: int64\n",
      "strict             32374\n",
      "flexible           22545\n",
      "moderate           19063\n",
      "super_strict_30      112\n",
      "super_strict_60       17\n",
      "Name: cancellation_policy, dtype: int64\n",
      "True     54403\n",
      "False    19708\n",
      "Name: cleaning_fee, dtype: int64\n",
      "NYC        32349\n",
      "LA         22453\n",
      "SF          6434\n",
      "DC          5688\n",
      "Chicago     3719\n",
      "Boston      3468\n",
      "Name: city, dtype: int64\n",
      "t    73697\n",
      "f      226\n",
      "Name: host_has_profile_pic, dtype: int64\n",
      "t    49748\n",
      "f    24175\n",
      "Name: host_identity_verified, dtype: int64\n",
      "f    54660\n",
      "t    19451\n",
      "Name: instant_bookable, dtype: int64\n",
      "Williamsburg          2862\n",
      "Bedford-Stuyvesant    2166\n",
      "Bushwick              1601\n",
      "Upper West Side       1396\n",
      "Mid-Wilshire          1392\n",
      "                      ... \n",
      "Arrochar                 1\n",
      "Woodland                 1\n",
      "Chillum, MD              1\n",
      "New Dorp Beach           1\n",
      "Mill Basin               1\n",
      "Name: neighbourhood, Length: 619, dtype: int64\n",
      "host_response_rate       18299\n",
      "review_scores_rating     16722\n",
      "first_review             15864\n",
      "last_review              15827\n",
      "neighbourhood             6872\n",
      "                         ...  \n",
      "Has_Doorman                  0\n",
      "Has_Wireless Internet        0\n",
      "Has_Host greets you          0\n",
      "Has_Essentials               0\n",
      "log_price                    0\n",
      "Length: 92, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# . . .\n",
    "cat_col = cat_col.drop(['id', 'zipcode'])\n",
    "df = df.drop(['id', 'zipcode'], axis=1)\n",
    "for i in cat_col:\n",
    "    print(df[i].value_counts())\n",
    "\n",
    "print(df.isna().sum().sort_values(ascending=False))\n",
    "\n",
    "df = df.drop(columns=['host_response_rate', 'review_scores_rating', 'first_review', 'last_review'])\n",
    "\n",
    "df = df.fillna(value=0)\n",
    "# . . ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3dEuVXUAG-kF"
   },
   "source": [
    "Ahora transformaremos todas las variables categoricas restantes a una representación en _one hot vector_. Para esto odemos utilizar la función `to_categorical` propuesta por keras. Puede apoyarse de las lineas de código abajo. No olvide eliminar las columnas originales del _dataframe_.\n",
    "\n",
    "Por último, en este caso optaremos por eliminar las columnas correspondientes a alguna fecha, pues no resultan significativas para el problema y la cantidad de variables disponibles son suficientes para hacer una exploración de las redes densas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "colab_type": "code",
    "id": "Ka-DhP_WG-kH",
    "outputId": "13203086-f7cb-4a84-ab3f-5602a8a55d64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marimo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/marimo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/marimo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/marimo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/marimo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/marimo/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>log_price</th>\n",
       "      <th>property_type</th>\n",
       "      <th>room_type</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bed_type</th>\n",
       "      <th>cancellation_policy</th>\n",
       "      <th>cleaning_fee</th>\n",
       "      <th>city</th>\n",
       "      <th>host_has_profile_pic</th>\n",
       "      <th>...</th>\n",
       "      <th>neighbourhood_610</th>\n",
       "      <th>neighbourhood_611</th>\n",
       "      <th>neighbourhood_612</th>\n",
       "      <th>neighbourhood_613</th>\n",
       "      <th>neighbourhood_614</th>\n",
       "      <th>neighbourhood_615</th>\n",
       "      <th>neighbourhood_616</th>\n",
       "      <th>neighbourhood_617</th>\n",
       "      <th>neighbourhood_618</th>\n",
       "      <th>neighbourhood_619</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.010635</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>strict</td>\n",
       "      <td>True</td>\n",
       "      <td>NYC</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.129899</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>strict</td>\n",
       "      <td>True</td>\n",
       "      <td>NYC</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.976734</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>moderate</td>\n",
       "      <td>True</td>\n",
       "      <td>NYC</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.620073</td>\n",
       "      <td>House</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>flexible</td>\n",
       "      <td>True</td>\n",
       "      <td>SF</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.744932</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>moderate</td>\n",
       "      <td>True</td>\n",
       "      <td>DC</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 771 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   log_price property_type        room_type  accommodates  bathrooms  \\\n",
       "0   5.010635     Apartment  Entire home/apt             3        1.0   \n",
       "1   5.129899     Apartment  Entire home/apt             7        1.0   \n",
       "2   4.976734     Apartment  Entire home/apt             5        1.0   \n",
       "3   6.620073         House  Entire home/apt             4        1.0   \n",
       "4   4.744932     Apartment  Entire home/apt             2        1.0   \n",
       "\n",
       "   bed_type cancellation_policy  cleaning_fee city host_has_profile_pic  ...  \\\n",
       "0  Real Bed              strict          True  NYC                    t  ...   \n",
       "1  Real Bed              strict          True  NYC                    t  ...   \n",
       "2  Real Bed            moderate          True  NYC                    t  ...   \n",
       "3  Real Bed            flexible          True   SF                    t  ...   \n",
       "4  Real Bed            moderate          True   DC                    t  ...   \n",
       "\n",
       "  neighbourhood_610 neighbourhood_611  neighbourhood_612  neighbourhood_613  \\\n",
       "0               0.0               0.0                0.0                0.0   \n",
       "1               0.0               0.0                0.0                0.0   \n",
       "2               0.0               0.0                0.0                0.0   \n",
       "3               0.0               0.0                0.0                0.0   \n",
       "4               0.0               0.0                0.0                0.0   \n",
       "\n",
       "  neighbourhood_614  neighbourhood_615  neighbourhood_616  neighbourhood_617  \\\n",
       "0               0.0                0.0                0.0                0.0   \n",
       "1               0.0                0.0                0.0                0.0   \n",
       "2               0.0                0.0                0.0                0.0   \n",
       "3               0.0                0.0                0.0                0.0   \n",
       "4               0.0                0.0                0.0                0.0   \n",
       "\n",
       "   neighbourhood_618  neighbourhood_619  \n",
       "0                0.0                0.0  \n",
       "1                0.0                0.0  \n",
       "2                0.0                0.0  \n",
       "3                0.0                0.0  \n",
       "4                0.0                0.0  \n",
       "\n",
       "[5 rows x 771 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "for col in cat_col:\n",
    "    onehot = to_categorical(df[col].astype('category').cat.codes)\n",
    "    df[[col + '_' + str(i) for i in range(onehot.shape[1])]] = pd.DataFrame(onehot)\n",
    "\n",
    "df = df.drop(['host_since'], axis=1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U1i3EzavG-kN"
   },
   "source": [
    "### 1.d Estandarización y Train Test Split\n",
    "En esta pregunta nos ocuparemos de separar el _dataset_ en los conjuntos de entrenamiento, validación y test y estandarizar los datos. Para esto puede utilizar la librería sklearn, en particular las funciones `StandarScaler` y `train_test_split`.\n",
    "\n",
    "Para esto separe primero el dataset en $X$ e $Y$. Luego separe los datos considerando un $70\\%$ de ellos para entrenamiento, un $20\\%$ para validación y un $10\\%$ para test. Finalmente ajuste los _scalers_ con los datos de entrenamiento y transforme los datos. \n",
    "\n",
    "- ¿Qué operación matemática realiza `StandarScaler` al momento de tranformar los datos? \n",
    "- ¿Por qué debemos transformar los datos de validación y de test con el _scaler_ ajustado a los datos de entrenamiento? \n",
    "- ¿Qué estamos tratando de representar en esta separación en conjuntos de entrenamiento, validación y test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "id": "7Q-NQWjfG-kQ",
    "outputId": "2f95c5c5-fbde-4235-82c3-c3b5df50d00e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['accommodates', 'bathrooms', 'latitude', 'longitude',\n",
      "       'number_of_reviews', 'bedrooms', 'beds', 'Has_Elevator',\n",
      "       'Has_Dishwasher', 'Has_Keypad',\n",
      "       ...\n",
      "       'neighbourhood_610', 'neighbourhood_611', 'neighbourhood_612',\n",
      "       'neighbourhood_613', 'neighbourhood_614', 'neighbourhood_615',\n",
      "       'neighbourhood_616', 'neighbourhood_617', 'neighbourhood_618',\n",
      "       'neighbourhood_619'],\n",
      "      dtype='object', length=760)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.drop(columns=['log_price'])\n",
    "X = X.drop(columns=cat_col)\n",
    "print(X.columns)\n",
    "\n",
    "y = df['log_price']\n",
    "\n",
    "X_train, X_aux, y_train, y_aux = train_test_split(X, y, test_size=0.30) \n",
    "X_val, X_test, y_val, y_test = train_test_split(X_aux, y_aux, test_size=0.66) \n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "scaler_x.fit(X_train)\n",
    "x_train = scaler_x.transform(X_train)\n",
    "x_test = scaler_x.transform(X_test)\n",
    "x_val = scaler_x.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kCtQEj21G-kV"
   },
   "source": [
    "### 1.e Primera Red\n",
    "En esta pregunta construiremos y entrenaremos una primera red neuronal. Para esto utilizaremos la librería keras que se ocupa de crear, compilar y entrenar los modelos de manera simple. Esta libreria puede conectarse a distintos _backend_ que proveen el _framework_ para realizar efectivamente las operaciones necesarias por la red. Los más usuales son _TensorFlow_ y _Theano_, sin embargo en el último tiempo _TensorFlow_ ha tenido una gran adopción por diversos motivos, por lo cual la recomendación es instalar y utilizar TensorFlow. Keras se encargará por lo tanto de crear los modelos y al momento de compilarlos se instanciaran estos en una sesión de TensorFlow. \n",
    "\n",
    "Esta primera red será una red de una capa oculta con $256$ neuronas, activación ReLu. Para esta red y todas las demas utilizaremos la función de pérdida _Mean Square Error_ para obtener resultados comparables entre distintos modelos. Para entrenar esta red utilizaremos Gradiente Descendente Estocástico con un _Learn Rate_ de 0.002. Finalmente entrenaremos esta red por unas 20 _epochs_. \n",
    "\n",
    "Construya la red basandose en el código y la documentación de keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iSlftir7G-kX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/marimo/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/marimo/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "inputs = Input(shape=(x_train.shape[1],))\n",
    "hidden1 = Dense(256, activation='relu')(inputs)\n",
    "outputs = Dense(1)(hidden1)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=SGD(lr=0.002), loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-HyHSEdcG-kb"
   },
   "source": [
    "Una forma fácil de instanciar la red es la propuesta en el codigo abajo, es decir entrenar la red por 0 _epochs_. Una red instanciada nos permite utilizar el método `.summary` para ver su número de parametros y los tamaños de cada capa. \n",
    "\n",
    "Explique el número de parámetros presentes en esta red, es decir: ¿Cómo a partir de la dimensión del _Input_ y el número de neuronas obtenemos ese número de parámetros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "colab_type": "code",
    "id": "lFFqmnq4G-kd",
    "outputId": "ddf1a33e-61df-4bfb-a361-f48f085d25ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/marimo/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 760)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               194816    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 195,073\n",
      "Trainable params: 195,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=0)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2hEhcGdRG-ki"
   },
   "source": [
    "Entrene la red por 20 _epochs_, guardando el `history` que retorna el metodo `.fit`.\n",
    "\n",
    "Grafique como varian los errores de validación y de entrenamiento a lo largo de las _epochs_. Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 998
    },
    "colab_type": "code",
    "id": "Ks8VgyxUG-kk",
    "outputId": "d1603c8a-46f3-4080-f5e5-8ab7a5db9dab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 5s 90us/sample - loss: 2.2710 - val_loss: 1.1267\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.5392 - val_loss: 0.4834\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 0.3467 - val_loss: 0.2992\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.2468 - val_loss: 0.2509\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 2s 43us/sample - loss: 0.2117 - val_loss: 0.2383\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 2s 43us/sample - loss: 0.1929 - val_loss: 0.2362\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 2s 43us/sample - loss: 0.1866 - val_loss: 0.1990\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 2s 43us/sample - loss: 0.1810 - val_loss: 0.1931\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 2s 42us/sample - loss: 0.1795 - val_loss: 0.1882\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 2s 43us/sample - loss: 0.1757 - val_loss: 0.1937\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1741 - val_loss: 0.1846\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.1753 - val_loss: 0.1843\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1732 - val_loss: 0.1837\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1719 - val_loss: 0.1822\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 2s 43us/sample - loss: 0.1699 - val_loss: 0.1840\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 2s 43us/sample - loss: 0.1680 - val_loss: 0.1945\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 2s 42us/sample - loss: 0.1675 - val_loss: 0.1818\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 2s 43us/sample - loss: 0.1667 - val_loss: 0.1905\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1658 - val_loss: 0.1880\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 2s 43us/sample - loss: 0.1652 - val_loss: 0.1819\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    validation_data=(x_val,y_val))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss']) \n",
    "plt.plot(history.history['val_loss']) \n",
    "plt.title('Model loss') \n",
    "plt.ylabel('Loss') \n",
    "plt.xlabel('Epoch') \n",
    "plt.legend(['Train', 'Test'], loc='upper left') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4HhlN0RBG-kp"
   },
   "source": [
    "Cree y entrene nuevamente la red, esta vez cambiando el _learn rate_ utilizado para el SGD. Pruebe a lo menos dos valores mayores y dos valores menores al elegido anteriormente. Note que para valores mayores al propuesto puede comenzar a observar fenómeno de divergencia, por lo cual es recomendable agregarle a la red un _calback_, es decir una función que verifica estados y comportamientos de la red mientras se entrena, en particular `TerminateOnNaN`, el cual interrumpirá el proceso de entrenamiento si encuentra un valor NaN. \n",
    "\n",
    "Grafique el comportamiento de los errores de validación y entrenamiento para a lo menos un valor mayor y uno menor al original y comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BiBTabGkG-kr",
    "outputId": "4fef7959-7ce5-48fd-d735-98098c5f6cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo con learning rate: 0.1\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "   32/51877 [..............................] - ETA: 3:40 - loss: 23.1134Batch 4: Invalid loss, terminating training\n",
      "Modelo detenido\n",
      "Entrenando modelo con learning rate: 0.01\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marimo/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py:370: RuntimeWarning: invalid value encountered in multiply\n",
      "  self._values[k][0] += v * (current - self._seen_so_far)\n",
      "/home/marimo/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/generic_utils.py:370: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  self._values[k][0] += v * (current - self._seen_so_far)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3744/51877 [=>............................] - ETA: 5s - loss: 18.3403Batch 129: Invalid loss, terminating training\n",
      "Modelo detenido\n",
      "Entrenando modelo con learning rate: 0.001\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 2.3644 - val_loss: 5.9227\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 2s 43us/sample - loss: 1.0678 - val_loss: 0.6107\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 2s 43us/sample - loss: 0.3600 - val_loss: 0.2338\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.2270 - val_loss: 0.2108\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 4s 68us/sample - loss: 0.2064 - val_loss: 0.2040\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.1986 - val_loss: 0.1991\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1936 - val_loss: 0.1964\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 3s 50us/sample - loss: 0.1902 - val_loss: 0.1993\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1870 - val_loss: 0.1915\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 3s 48us/sample - loss: 0.1843 - val_loss: 0.1917\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1820 - val_loss: 0.1886\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1799 - val_loss: 0.1918\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.1782 - val_loss: 0.1883\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1765 - val_loss: 0.1851\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1748 - val_loss: 0.1839\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1736 - val_loss: 0.1828\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.1723 - val_loss: 0.1828\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.1710 - val_loss: 0.1820\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.1699 - val_loss: 0.1815\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.1688 - val_loss: 0.1804\n",
      "Entrenando modelo con learning rate: 0.0001\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 7.3082 - val_loss: 3.7807\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 2.2916 - val_loss: 1.1901\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.8273 - val_loss: 0.5728\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.4828 - val_loss: 0.4209\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.3872 - val_loss: 0.3702\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.3501 - val_loss: 0.3473\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.3303 - val_loss: 0.3334\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.3174 - val_loss: 0.3234\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.3075 - val_loss: 0.3159\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.2993 - val_loss: 0.3094\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.2925 - val_loss: 0.3048\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.2866 - val_loss: 0.2995\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.2812 - val_loss: 0.2947\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.2764 - val_loss: 0.2908\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.2720 - val_loss: 0.2872\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.2680 - val_loss: 0.2838\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.2642 - val_loss: 0.2809\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.2608 - val_loss: 0.2781\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.2576 - val_loss: 0.2753\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.2546 - val_loss: 0.2733\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TerminateOnNaN\n",
    "\n",
    "lrs = [0.1, 0.01, 0.001, 0.0001]\n",
    "terminate = TerminateOnNaN()\n",
    "\n",
    "for n, lr in enumerate(lrs):\n",
    "  print('Entrenando modelo con learning rate:', lr)\n",
    "  inputs = Input(shape=(x_train.shape[1],))\n",
    "  hidden1 = Dense(256, activation='relu')(inputs)\n",
    "  outputs = Dense(1)(hidden1)\n",
    "  model = Model(inputs=inputs, outputs=outputs)\n",
    "  model.compile(optimizer=SGD(lr=lr), loss='mse')\n",
    "  \n",
    "  history = model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val), callbacks=[terminate])\n",
    "\n",
    "  try:\n",
    "    plt.subplot(2, 2, n+1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "  except KeyError:\n",
    "    print('Modelo detenido')\n",
    "  \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RYOwa3NMG-kw"
   },
   "source": [
    "### 1.f Activación y regularizadores $l$1 $l$2\n",
    "En esta pregunta se les propone explorar distintas funciones de activación o de regularización usual. Por lo extenso de la tarea se les propone elegir una de las dos exploraciones. En ambos casos deben entrenar la misma red entrenada anteriormente utilizando gradiente descendente con algun _learn rate_ que les parezca adecuado luego de la exploración en la pregunta anterior. \n",
    "\n",
    "* En caso de elegir explorar distintas funciones de activación, cambie la activación de la capa oculta sucesivamente por: tangente hiperbólica, _Leaky ReLu_, _softmax_, sigmoidea y lineal. Para esto puede basarse en el código presentado abajo y la documentación de keras. Para la activación _Leaky ReLu_ pruebe cambiar el parámetro de la red. Describa sus resultados y si observa diferencias entre las redes. \n",
    "\n",
    "* En caso de elegir explorar las funciones de regularización usual, agregue regularización $l$1 o $l$2 a la capa oculta y pruebe cambiar la tasa de regularización, reportando sus resultados. ¿Qué ocurre si la regularización es muy alta o muy baja? Una vez satisfecho con una tasa de regularización, aplique la regularización a la capa de salida y luego a ambas capas. \n",
    "\n",
    "**Independiente de la opción elegida**, comente sobre los siguientes temas:\n",
    "\n",
    "¿Cual es el interez de tener activaciones no lineales? ¿Le parece buena opción la activación sigmoidea para la capa oculta? ¿Qué pasaría si usaramos esta activación en la capa de salida? \n",
    "\n",
    "¿Cual es la intención de la regularización en general? En particular, ¿Que restricción implicita imponen las regularizaciones $l$1 o $l$2 sobre los pesos de la capa en la cual se aplican? Apoyese de ecuaciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0Tojo9i6G-kx",
    "outputId": "f1574f3e-f9f8-452f-9304-d80b3c77a88a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 2.0812 - val_loss: 0.2807\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 0.2478 - val_loss: 0.2491\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.2258 - val_loss: 0.2387\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.2152 - val_loss: 0.2316\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.2080 - val_loss: 0.2284\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 3s 50us/sample - loss: 0.2023 - val_loss: 0.2257\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1970 - val_loss: 0.2229\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.1929 - val_loss: 0.2199\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1889 - val_loss: 0.2175\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1855 - val_loss: 0.2174\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1828 - val_loss: 0.2149\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.1799 - val_loss: 0.2117\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.1774 - val_loss: 0.2126\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.1752 - val_loss: 0.2104\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.1727 - val_loss: 0.2081\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1710 - val_loss: 0.2089\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1690 - val_loss: 0.2050\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1672 - val_loss: 0.2060\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1655 - val_loss: 0.2049\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.1641 - val_loss: 0.2045\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 3s 50us/sample - loss: 4.0116 - val_loss: 0.5483\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.5215 - val_loss: 0.5145\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.5161 - val_loss: 0.5139\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.5157 - val_loss: 0.5135\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.5153 - val_loss: 0.5131\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.5149 - val_loss: 0.5127\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.5145 - val_loss: 0.5123\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.5142 - val_loss: 0.5119\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.5138 - val_loss: 0.5115\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.5134 - val_loss: 0.5111\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.5130 - val_loss: 0.5108\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 3s 48us/sample - loss: 0.5126 - val_loss: 0.5103\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.5121 - val_loss: 0.5099\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.5117 - val_loss: 0.5095\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.5113 - val_loss: 0.5091\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.5109 - val_loss: 0.5087\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 3s 48us/sample - loss: 0.5105 - val_loss: 0.5083\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.5101 - val_loss: 0.5078\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.5096 - val_loss: 0.5074\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.5092 - val_loss: 0.5070\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.4554 - val_loss: 0.2791\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.2532 - val_loss: 0.2277\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.2202 - val_loss: 0.2065\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.2045 - val_loss: 0.1956\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1954 - val_loss: 0.1899\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1900 - val_loss: 0.1880\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1864 - val_loss: 0.1847\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1839 - val_loss: 0.1818\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1822 - val_loss: 0.1820\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1809 - val_loss: 0.1795\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1799 - val_loss: 0.1841\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1791 - val_loss: 0.1794\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1785 - val_loss: 0.1787\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1779 - val_loss: 0.1786\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1775 - val_loss: 0.1798\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1771 - val_loss: 0.1776\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 3s 66us/sample - loss: 0.1767 - val_loss: 0.1776\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.1764 - val_loss: 0.1773\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 4s 69us/sample - loss: 0.1762 - val_loss: 0.1772\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.1759 - val_loss: 0.1772\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 3.5279 - val_loss: 10.4713\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 2.1420 - val_loss: 3.0575\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 1.0010 - val_loss: 0.9424\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.5247 - val_loss: 0.3450\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.2945 - val_loss: 0.2500\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.2187 - val_loss: 0.2139\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1971 - val_loss: 0.1978\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.1842 - val_loss: 0.1899\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 3s 63us/sample - loss: 0.1788 - val_loss: 0.1771\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 3s 65us/sample - loss: 0.1755 - val_loss: 0.1779\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.1758 - val_loss: 0.1759\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1748 - val_loss: 0.1761\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 3s 50us/sample - loss: 0.1749 - val_loss: 0.1767\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1745 - val_loss: 0.1759\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1742 - val_loss: 0.1776\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1741 - val_loss: 0.1747\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1742 - val_loss: 0.1759\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1742 - val_loss: 0.1762\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1751 - val_loss: 0.1774\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.1745 - val_loss: 0.1757\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 3s 50us/sample - loss: 2.4099 - val_loss: 2.5810\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.9063 - val_loss: 0.7126\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.3445 - val_loss: 0.3440\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 3s 50us/sample - loss: 0.2333 - val_loss: 0.2187\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.2031 - val_loss: 0.2062\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1942 - val_loss: 0.1996\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1899 - val_loss: 0.1966\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1870 - val_loss: 0.1947\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1844 - val_loss: 0.1928\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1823 - val_loss: 0.1907\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1802 - val_loss: 0.1899\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1785 - val_loss: 0.1894\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1770 - val_loss: 0.1883\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1757 - val_loss: 0.1877\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 3s 48us/sample - loss: 0.1743 - val_loss: 0.1873\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.1732 - val_loss: 0.1873\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.1720 - val_loss: 0.1856\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 2s 43us/sample - loss: 0.1709 - val_loss: 0.1845\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.1700 - val_loss: 0.1844\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.1691 - val_loss: 0.1827\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 2.5446 - val_loss: 4.9967\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 1.3539 - val_loss: 0.8500\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.7147 - val_loss: 0.3546\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.3270 - val_loss: 0.2683\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.2158 - val_loss: 0.2206\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1965 - val_loss: 0.1995\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1843 - val_loss: 0.1837\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1802 - val_loss: 0.1813\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1787 - val_loss: 0.1817\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.1778 - val_loss: 0.1817\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.1774 - val_loss: 0.1821\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1768 - val_loss: 0.1813\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 3s 48us/sample - loss: 0.1764 - val_loss: 0.1806\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1759 - val_loss: 0.1801\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1756 - val_loss: 0.1794\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1752 - val_loss: 0.1793\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 3s 50us/sample - loss: 0.1751 - val_loss: 0.1791\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1745 - val_loss: 0.1793\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1742 - val_loss: 0.1787\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1739 - val_loss: 0.1784\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 3.2565 - val_loss: 15.9459\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 2.9076 - val_loss: 6.2406\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 1.5307 - val_loss: 2.9675\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 10.6799 - val_loss: 314.5209\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 3s 50us/sample - loss: nan - val_loss: nan\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: nan - val_loss: nan\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 2s 43us/sample - loss: nan - val_loss: nan\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: nan - val_loss: nan\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: nan - val_loss: nan\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: nan - val_loss: nan\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: nan - val_loss: nan\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: nan - val_loss: nan\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: nan - val_loss: nan\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: nan - val_loss: nan\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 7 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Activation, LeakyReLU\n",
    "\n",
    "activations = [Activation(i) for i in ['tanh', 'softmax', 'sigmoid', None]]\n",
    "activations += [LeakyReLU(i) for i in [0.1, 0.5, 0.9]]\n",
    "\n",
    "for n, activation in enumerate(activations):\n",
    "  inputs = Input(shape=(x_train.shape[1],))\n",
    "  hidden1 = Dense(256)(inputs)\n",
    "  activ1 = activation(hidden1)\n",
    "  outputs = Dense(1)(activ1)\n",
    "  model = Model(inputs=inputs, outputs=outputs)\n",
    "  model.compile(optimizer=SGD(lr=0.001), loss='mse')\n",
    "\n",
    "  history = model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val), callbacks=[terminate])\n",
    "\n",
    "  plt.subplot(2, 4, n+1)\n",
    "  plt.plot(history.history['loss'])\n",
    "  plt.plot(history.history['val_loss'])\n",
    "  plt.title('Model loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Test'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdrih9MOG-k1"
   },
   "source": [
    "### 1.g Exploración del Número de neuronas\n",
    "\n",
    "Ahora probaremos cambiando el número de neuronas en la capa oculta. Para esto, entrenen la red que estimen conveniente luego de la pregunta anterior, variando el numero de nuronas. Deben explorar a lo menos 10 número de neuronas distintos. Una recomendación sería por ejemplo explorar numero de neuronas en potencias de 2. \n",
    "\n",
    "Para cada red entrenada, recuperen el mejor error de validación y el error de entrenamiento en la _epoch_ donde se obtuvo tal error de validación. Grafique como se comportan ambos errores a medida crece el número de neuronas y comente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "deJL6amgG-k3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando con 1  neuronas.\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 3.4382 - val_loss: 3.0496\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.9258 - val_loss: 0.5128\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.3462 - val_loss: 0.3432\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.2735 - val_loss: 0.2950\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.2426 - val_loss: 0.2627\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.2257 - val_loss: 0.2364\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.2126 - val_loss: 0.2228\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.2039 - val_loss: 0.2109\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 3s 61us/sample - loss: 0.1970 - val_loss: 0.2026\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 3s 48us/sample - loss: 0.1919 - val_loss: 0.1971\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1880 - val_loss: 0.1917\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1849 - val_loss: 0.1883\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1825 - val_loss: 0.1852\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 3s 50us/sample - loss: 0.1806 - val_loss: 0.1829\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 3s 48us/sample - loss: 0.1791 - val_loss: 0.1811\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1779 - val_loss: 0.1799\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1769 - val_loss: 0.1788\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1761 - val_loss: 0.1779\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1755 - val_loss: 0.1775\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1750 - val_loss: 0.1769\n",
      "Entrenando con 2  neuronas.\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 2.7240 - val_loss: 2.7585\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 3s 48us/sample - loss: 1.0306 - val_loss: 0.5099\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.3300 - val_loss: 0.2708\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.2024 - val_loss: 0.1965\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1850 - val_loss: 0.1843\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 3s 48us/sample - loss: 0.1786 - val_loss: 0.1790\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1753 - val_loss: 0.1760\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1737 - val_loss: 0.1751\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1727 - val_loss: 0.1747\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1723 - val_loss: 0.1740\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1719 - val_loss: 0.1738\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1718 - val_loss: 0.1740\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1716 - val_loss: 0.1738\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1716 - val_loss: 0.1740\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1715 - val_loss: 0.1740\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.1714 - val_loss: 0.1737\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1714 - val_loss: 0.1738\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 3s 48us/sample - loss: 0.1714 - val_loss: 0.1740\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1713 - val_loss: 0.1738\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1714 - val_loss: 0.1737\n",
      "Entrenando con 4  neuronas.\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 3s 63us/sample - loss: 2.7322 - val_loss: 3.2883\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 1.0379 - val_loss: 0.3081\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.4822 - val_loss: 0.1984\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 3s 50us/sample - loss: 0.2340 - val_loss: 0.1836\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1986 - val_loss: 0.1788\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1853 - val_loss: 0.1758\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1787 - val_loss: 0.1761\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.1753 - val_loss: 0.1739\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.1736 - val_loss: 0.1740\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.1726 - val_loss: 0.1734\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.1720 - val_loss: 0.1735\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1716 - val_loss: 0.1733\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1714 - val_loss: 0.1732\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1712 - val_loss: 0.1731\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 3s 63us/sample - loss: 0.1711 - val_loss: 0.1729\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1710 - val_loss: 0.1732\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1709 - val_loss: 0.1735\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.1708 - val_loss: 0.1731\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1708 - val_loss: 0.1728\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.1707 - val_loss: 0.1731\n",
      "Entrenando con 8  neuronas.\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 4s 78us/sample - loss: 2.3166 - val_loss: 3.7968\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 1.2469 - val_loss: 1.3842\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.4667 - val_loss: 0.5913\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 3s 50us/sample - loss: 0.2494 - val_loss: 0.2812\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 3s 50us/sample - loss: 0.2003 - val_loss: 0.2234\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 3s 67us/sample - loss: 0.1855 - val_loss: 0.1978\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1783 - val_loss: 0.1864\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.1754 - val_loss: 0.1799\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.1741 - val_loss: 0.1766\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.1733 - val_loss: 0.1755\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.1729 - val_loss: 0.1754\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 3s 48us/sample - loss: 0.1726 - val_loss: 0.1755\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.1724 - val_loss: 0.1751\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1723 - val_loss: 0.1748\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 2s 43us/sample - loss: 0.1722 - val_loss: 0.1749\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 2s 43us/sample - loss: 0.1720 - val_loss: 0.1749\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.1718 - val_loss: 0.1745\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1717 - val_loss: 0.1748\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 3s 50us/sample - loss: 0.1716 - val_loss: 0.1745\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1715 - val_loss: 0.1746\n",
      "Entrenando con 16  neuronas.\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 2.3778 - val_loss: 2.0324\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 1.2273 - val_loss: 0.4712\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.3595 - val_loss: 0.2261\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.2021 - val_loss: 0.1857\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1820 - val_loss: 0.1815\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1786 - val_loss: 0.1808\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1775 - val_loss: 0.1800\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1769 - val_loss: 0.1793\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1762 - val_loss: 0.1787\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1758 - val_loss: 0.1787\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1754 - val_loss: 0.1782\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1750 - val_loss: 0.1780\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1747 - val_loss: 0.1781\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1743 - val_loss: 0.1773\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1740 - val_loss: 0.1772\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.1738 - val_loss: 0.1768\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1735 - val_loss: 0.1773\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.1732 - val_loss: 0.1767\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 0.1730 - val_loss: 0.1762\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1728 - val_loss: 0.1767\n",
      "Entrenando con 32  neuronas.\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 2.3030 - val_loss: 3.2521\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 2s 45us/sample - loss: 1.0414 - val_loss: 0.4330\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.2899 - val_loss: 0.2239\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 2s 43us/sample - loss: 0.1896 - val_loss: 0.1840\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1800 - val_loss: 0.1820\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1785 - val_loss: 0.1821\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.1778 - val_loss: 0.1806\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1771 - val_loss: 0.1807\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 3s 50us/sample - loss: 0.1766 - val_loss: 0.1802\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1761 - val_loss: 0.1797\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1758 - val_loss: 0.1796\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1753 - val_loss: 0.1793\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 3s 63us/sample - loss: 0.1749 - val_loss: 0.1786\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1746 - val_loss: 0.1786\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 3s 48us/sample - loss: 0.1743 - val_loss: 0.1783\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 3s 48us/sample - loss: 0.1739 - val_loss: 0.1780\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1736 - val_loss: 0.1778\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1733 - val_loss: 0.1780\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1730 - val_loss: 0.1778\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 3s 48us/sample - loss: 0.1728 - val_loss: 0.1776\n",
      "Entrenando con 64  neuronas.\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 2.2639 - val_loss: 2.6702\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 3s 48us/sample - loss: 1.2423 - val_loss: 0.5910\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.4015 - val_loss: 0.2818\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.2153 - val_loss: 0.1869\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 3s 48us/sample - loss: 0.1827 - val_loss: 0.1817\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1786 - val_loss: 0.1809\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 3s 48us/sample - loss: 0.1776 - val_loss: 0.1805\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1769 - val_loss: 0.1802\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1764 - val_loss: 0.1797\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1760 - val_loss: 0.1791\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1756 - val_loss: 0.1793\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.1752 - val_loss: 0.1793\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1748 - val_loss: 0.1786\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.1744 - val_loss: 0.1787\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1741 - val_loss: 0.1785\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1738 - val_loss: 0.1782\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 3s 63us/sample - loss: 0.1735 - val_loss: 0.1777\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 0.1733 - val_loss: 0.1780\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1729 - val_loss: 0.1771\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 0.1727 - val_loss: 0.1771\n",
      "Entrenando con 128  neuronas.\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 2.3394 - val_loss: 5.5975\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 1.3300 - val_loss: 0.3610\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.4111 - val_loss: 0.2110\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.2048 - val_loss: 0.1912\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.1825 - val_loss: 0.1837\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 0.1799 - val_loss: 0.1821\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.1787 - val_loss: 0.1797\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.1779 - val_loss: 0.1803\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.1772 - val_loss: 0.1806\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.1769 - val_loss: 0.1789\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.1763 - val_loss: 0.1796\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1759 - val_loss: 0.1792\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 3s 48us/sample - loss: 0.1755 - val_loss: 0.1786\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1752 - val_loss: 0.1785\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 2s 48us/sample - loss: 0.1747 - val_loss: 0.1783\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.1743 - val_loss: 0.1778\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 3s 49us/sample - loss: 0.1742 - val_loss: 0.1781\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.1738 - val_loss: 0.1771\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1734 - val_loss: 0.1773\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 0.1731 - val_loss: 0.1772\n",
      "Entrenando con 256  neuronas.\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 4s 69us/sample - loss: 2.5725 - val_loss: 6.6344\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 1.3365 - val_loss: 1.2609\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 3s 63us/sample - loss: 0.7334 - val_loss: 0.5103\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.4322 - val_loss: 0.2672\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 3s 63us/sample - loss: 0.2631 - val_loss: 0.2199\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.2016 - val_loss: 0.1960\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 4s 73us/sample - loss: 0.1840 - val_loss: 0.1840\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 3s 63us/sample - loss: 0.1785 - val_loss: 0.1811\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 3s 61us/sample - loss: 0.1772 - val_loss: 0.1805\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1765 - val_loss: 0.1796\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1760 - val_loss: 0.1808\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 2s 46us/sample - loss: 0.1757 - val_loss: 0.1798\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1752 - val_loss: 0.1791\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1748 - val_loss: 0.1788\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 2s 44us/sample - loss: 0.1745 - val_loss: 0.1795\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 2s 42us/sample - loss: 0.1742 - val_loss: 0.1803\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 2s 43us/sample - loss: 0.1738 - val_loss: 0.1797\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.1735 - val_loss: 0.1782\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 4s 69us/sample - loss: 0.1732 - val_loss: 0.1783\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 3s 62us/sample - loss: 0.1729 - val_loss: 0.1770\n",
      "Entrenando con 512  neuronas.\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 4s 81us/sample - loss: 2.7732 - val_loss: 11.5604\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 3s 62us/sample - loss: 1.6120 - val_loss: 3.5532\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 1.2121 - val_loss: 1.6698\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.6769 - val_loss: 0.7918\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.5062 - val_loss: 0.4960\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 4s 69us/sample - loss: 0.4286 - val_loss: 0.3384\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 4s 71us/sample - loss: 0.2865 - val_loss: 0.2999\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 4s 78us/sample - loss: 0.2305 - val_loss: 0.2187\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 4s 75us/sample - loss: 0.1926 - val_loss: 0.1921\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 4s 73us/sample - loss: 0.1807 - val_loss: 0.1798\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 3s 66us/sample - loss: 0.1771 - val_loss: 0.1795\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.1759 - val_loss: 0.1767\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 3s 67us/sample - loss: 0.1749 - val_loss: 0.1772\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.1745 - val_loss: 0.1768\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.1744 - val_loss: 0.1760\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.1743 - val_loss: 0.1772\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 3s 66us/sample - loss: 0.1740 - val_loss: 0.1764\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.1736 - val_loss: 0.1761\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.1732 - val_loss: 0.1767\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 4s 70us/sample - loss: 0.1731 - val_loss: 0.1766\n",
      "Entrenando con 1024  neuronas.\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 4s 79us/sample - loss: 3.4717 - val_loss: 20.7705\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 2.3503 - val_loss: 5.6471\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 1.4942 - val_loss: 5.9914\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 1.1909 - val_loss: 2.8043\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 1.2553 - val_loss: 5.4461\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 1.2512 - val_loss: 6.3409\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 1.1095 - val_loss: 3.4057\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 1.0556 - val_loss: 3.4688\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 1.2638 - val_loss: 6.0341\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 1.2162 - val_loss: 2.9509\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 2.4824 - val_loss: 12.1642\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 3.0543 - val_loss: 11.4183\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 14.5596 - val_loss: 11.0904\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: nan - val_loss: nan\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: nan - val_loss: nan\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: nan - val_loss: nan\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: nan - val_loss: nan\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: nan - val_loss: nan\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: nan - val_loss: nan\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: nan - val_loss: nan\n",
      "Entrenando con 2048  neuronas.\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51877/51877 [==============================] - 5s 95us/sample - loss: 4.1192 - val_loss: 29.8184\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 4s 74us/sample - loss: 9.4747 - val_loss: 18.3989\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 4s 77us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 4s 74us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 4s 83us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 4s 81us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 5s 93us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 5s 98us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 4s 81us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 5s 93us/sample - loss: nan - val_loss: nan\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 4s 85us/sample - loss: nan - val_loss: nan\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 4s 76us/sample - loss: nan - val_loss: nan\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 4s 75us/sample - loss: nan - val_loss: nan\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: nan - val_loss: nan\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 5s 99us/sample - loss: nan - val_loss: nan\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 5s 94us/sample - loss: nan - val_loss: nan\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 4s 76us/sample - loss: nan - val_loss: nan\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 5s 87us/sample - loss: nan - val_loss: nan\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 5s 87us/sample - loss: nan - val_loss: nan\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 6s 108us/sample - loss: nan - val_loss: nan\n",
      "Entrenando con 4096  neuronas.\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 7s 143us/sample - loss: 4.6775 - val_loss: 20.9313\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 7s 142us/sample - loss: 8.1579 - val_loss: 7.3759\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 6s 120us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 6s 117us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 6s 119us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 6s 111us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 6s 114us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 6s 116us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 6s 118us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 6s 108us/sample - loss: nan - val_loss: nan\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 5s 104us/sample - loss: nan - val_loss: nan\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 5s 104us/sample - loss: nan - val_loss: nan\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 5s 104us/sample - loss: nan - val_loss: nan\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 5s 104us/sample - loss: nan - val_loss: nan\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 5s 105us/sample - loss: nan - val_loss: nan\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 5s 103us/sample - loss: nan - val_loss: nan\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 7s 130us/sample - loss: nan - val_loss: nan\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 6s 118us/sample - loss: nan - val_loss: nan\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 6s 114us/sample - loss: nan - val_loss: nan\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 6s 108us/sample - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_error, val_error = [], []\n",
    "\n",
    "for n_units in [2**i for i in range(13)]:\n",
    "    print('Entrenando con', n_units, ' neuronas.')\n",
    "    inputs = Input(shape=(x_train.shape[1],))\n",
    "    hidden1 = Dense(n_units)(inputs)\n",
    "    activ1 = LeakyReLU(0.5)(hidden1)\n",
    "    outputs = Dense(1)(activ1)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=SGD(lr=0.001), loss='mse')\n",
    "\n",
    "    history = model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val), callbacks=[terminate])\n",
    "    \n",
    "    val_error.append(min(history.history['val_loss']))\n",
    "    train_error.append(history.history['loss'][np.argmin(history.history['val_loss'])])\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot([2**i for i in range(13)], train_error)\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot([2**i for i in range(13)], val_error)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-9Z2j4uiG-k8"
   },
   "source": [
    "### 1.h Dropout\n",
    "Como seguramente constataron en la pregunta anterior, un numero demasiado grande de parámetros en el modelo puede llevarnos a observar el fenomeno de _overfitting_. Una aproximación a este fenómeno que ha dado excelente resultado en redes neuronales es el método _dropout_, donde estocásticamente se desactivan una fracción de las neuronas al momento del entrenamiento, así efectivamente reduciendo el tamaño del modelo que se entrena en cada iteración e implicitamente obteniendo modelos más robustos por el simple hecho que al momento de entrenar nunca se entrena el \"mismo\" modelo. \n",
    "\n",
    "Según lo aprendido en el ramo, ¿en qué consiste el fenómeno de _overfitting_? ¿Por qué modelos más grandes suelen presentar el fenómeno? \n",
    "\n",
    "Entrene la mejor red obtenida en la pregunta anterior agregando una capa de _Dropout_ con parámetro $0.5$ inmediatamente luego de la capa oculta. Repita luego el proceso con una red con el doble de neuronas. Note que el agregar una capa _dropout_ hará que la red entrene más lento, por lo cual es recomendable aumentar el numero de _epochs_ para entrenar la red a completitud. \n",
    "\n",
    "¿Qué observa al agregar _dropout_? Comente y compare con sus resultados anteriores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdFSR-dEG-k9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando con 64 neuronas\n",
      "WARNING:tensorflow:From /home/marimo/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/120\n",
      "51877/51877 [==============================] - 4s 82us/sample - loss: 3.5543 - val_loss: 0.5324\n",
      "Epoch 2/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.8839 - val_loss: 1.6173\n",
      "Epoch 3/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.5638 - val_loss: 0.4117\n",
      "Epoch 4/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.4275 - val_loss: 0.2216\n",
      "Epoch 5/120\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.2959 - val_loss: 0.2261\n",
      "Epoch 6/120\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.2859 - val_loss: 0.2158\n",
      "Epoch 7/120\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.2699 - val_loss: 0.2055\n",
      "Epoch 8/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.2639 - val_loss: 0.2102\n",
      "Epoch 9/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.2550 - val_loss: 0.1958\n",
      "Epoch 10/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2530 - val_loss: 0.1934\n",
      "Epoch 11/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2442 - val_loss: 0.1954\n",
      "Epoch 12/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2446 - val_loss: 0.1915\n",
      "Epoch 13/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2392 - val_loss: 0.1918\n",
      "Epoch 14/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2356 - val_loss: 0.1874\n",
      "Epoch 15/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.2320 - val_loss: 0.1854\n",
      "Epoch 16/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.2301 - val_loss: 0.1870\n",
      "Epoch 17/120\n",
      "51877/51877 [==============================] - 3s 62us/sample - loss: 0.2306 - val_loss: 0.1849\n",
      "Epoch 18/120\n",
      "51877/51877 [==============================] - 3s 67us/sample - loss: 0.2288 - val_loss: 0.1912\n",
      "Epoch 19/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.2240 - val_loss: 0.1863\n",
      "Epoch 20/120\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.2261 - val_loss: 0.1856\n",
      "Epoch 21/120\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.2271 - val_loss: 0.1813\n",
      "Epoch 22/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.2212 - val_loss: 0.1827\n",
      "Epoch 23/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.2175 - val_loss: 0.1809\n",
      "Epoch 24/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2181 - val_loss: 0.1826\n",
      "Epoch 25/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2192 - val_loss: 0.1802\n",
      "Epoch 26/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2168 - val_loss: 0.1810\n",
      "Epoch 27/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.2164 - val_loss: 0.1845\n",
      "Epoch 28/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.2157 - val_loss: 0.1795\n",
      "Epoch 29/120\n",
      "51877/51877 [==============================] - 3s 64us/sample - loss: 0.2167 - val_loss: 0.1815\n",
      "Epoch 30/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.2171 - val_loss: 0.2379\n",
      "Epoch 31/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.2296 - val_loss: 0.1801\n",
      "Epoch 32/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.2121 - val_loss: 0.1795\n",
      "Epoch 33/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2102 - val_loss: 0.1786\n",
      "Epoch 34/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2121 - val_loss: 0.1814\n",
      "Epoch 35/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2137 - val_loss: 0.1810\n",
      "Epoch 36/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2203 - val_loss: 0.1797\n",
      "Epoch 37/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2093 - val_loss: 0.1796\n",
      "Epoch 38/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.2094 - val_loss: 0.1815\n",
      "Epoch 39/120\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.2097 - val_loss: 0.1778\n",
      "Epoch 40/120\n",
      "51877/51877 [==============================] - 3s 63us/sample - loss: 0.2073 - val_loss: 0.1767\n",
      "Epoch 41/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2087 - val_loss: 0.1798\n",
      "Epoch 42/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2081 - val_loss: 0.1779\n",
      "Epoch 43/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2144 - val_loss: 0.1795\n",
      "Epoch 44/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2147 - val_loss: 0.1767\n",
      "Epoch 45/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2078 - val_loss: 0.1771\n",
      "Epoch 46/120\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.2073 - val_loss: 0.1760\n",
      "Epoch 47/120\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.2082 - val_loss: 0.1766\n",
      "Epoch 48/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.2068 - val_loss: 0.1771\n",
      "Epoch 49/120\n",
      "51877/51877 [==============================] - 3s 65us/sample - loss: 0.2049 - val_loss: 0.1796\n",
      "Epoch 50/120\n",
      "51877/51877 [==============================] - 3s 61us/sample - loss: 0.2044 - val_loss: 0.1787\n",
      "Epoch 51/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2066 - val_loss: 0.1771\n",
      "Epoch 52/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.2057 - val_loss: 0.1776\n",
      "Epoch 53/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2054 - val_loss: 0.1785\n",
      "Epoch 54/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.2087 - val_loss: 0.1751\n",
      "Epoch 55/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.2032 - val_loss: 0.1754\n",
      "Epoch 56/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.2027 - val_loss: 0.1773\n",
      "Epoch 57/120\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.2027 - val_loss: 0.1774\n",
      "Epoch 58/120\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.2075 - val_loss: 0.1849\n",
      "Epoch 59/120\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.2048 - val_loss: 0.1776\n",
      "Epoch 60/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.2054 - val_loss: 0.1758\n",
      "Epoch 61/120\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.2037 - val_loss: 0.1829\n",
      "Epoch 62/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.2024 - val_loss: 0.1767\n",
      "Epoch 63/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.2027 - val_loss: 0.1815\n",
      "Epoch 64/120\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.2000 - val_loss: 0.1748\n",
      "Epoch 65/120\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.2003 - val_loss: 0.1767\n",
      "Epoch 66/120\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.2041 - val_loss: 0.1773\n",
      "Epoch 67/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.2009 - val_loss: 0.1764\n",
      "Epoch 68/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.1990 - val_loss: 0.1771\n",
      "Epoch 69/120\n",
      "51877/51877 [==============================] - 3s 61us/sample - loss: 0.1986 - val_loss: 0.1758\n",
      "Epoch 70/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.2014 - val_loss: 0.1769\n",
      "Epoch 71/120\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.2008 - val_loss: 0.1747\n",
      "Epoch 72/120\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.2010 - val_loss: 0.1757\n",
      "Epoch 73/120\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.1974 - val_loss: 0.1754\n",
      "Epoch 74/120\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.1981 - val_loss: 0.1750\n",
      "Epoch 75/120\n",
      "51877/51877 [==============================] - 4s 73us/sample - loss: 0.1990 - val_loss: 0.1748\n",
      "Epoch 76/120\n",
      "51877/51877 [==============================] - 3s 63us/sample - loss: 0.1976 - val_loss: 0.1749\n",
      "Epoch 77/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1991 - val_loss: 0.1776\n",
      "Epoch 78/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.2001 - val_loss: 0.1788\n",
      "Epoch 79/120\n",
      "51877/51877 [==============================] - 3s 50us/sample - loss: 0.1977 - val_loss: 0.1762\n",
      "Epoch 80/120\n",
      "51877/51877 [==============================] - 3s 50us/sample - loss: 0.1967 - val_loss: 0.1769\n",
      "Epoch 81/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1995 - val_loss: 0.1744\n",
      "Epoch 82/120\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.1987 - val_loss: 0.1740\n",
      "Epoch 83/120\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.1982 - val_loss: 0.1763\n",
      "Epoch 84/120\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.1958 - val_loss: 0.1746\n",
      "Epoch 85/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1986 - val_loss: 0.1756\n",
      "Epoch 86/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1963 - val_loss: 0.1753\n",
      "Epoch 87/120\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.1971 - val_loss: 0.1782\n",
      "Epoch 88/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1959 - val_loss: 0.1745\n",
      "Epoch 89/120\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.1961 - val_loss: 0.1745\n",
      "Epoch 90/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1956 - val_loss: 0.1749\n",
      "Epoch 91/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1968 - val_loss: 0.1750\n",
      "Epoch 92/120\n",
      "51877/51877 [==============================] - 3s 50us/sample - loss: 0.1961 - val_loss: 0.1733\n",
      "Epoch 93/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.1969 - val_loss: 0.1763\n",
      "Epoch 94/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1966 - val_loss: 0.1807\n",
      "Epoch 95/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1957 - val_loss: 0.1742\n",
      "Epoch 96/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1953 - val_loss: 0.1745\n",
      "Epoch 97/120\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.1954 - val_loss: 0.1748\n",
      "Epoch 98/120\n",
      "51877/51877 [==============================] - 3s 50us/sample - loss: 0.1972 - val_loss: 0.1743\n",
      "Epoch 99/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1971 - val_loss: 0.1742\n",
      "Epoch 100/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1948 - val_loss: 0.1736\n",
      "Epoch 101/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1939 - val_loss: 0.1748\n",
      "Epoch 102/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1955 - val_loss: 0.1762\n",
      "Epoch 103/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1944 - val_loss: 0.1731\n",
      "Epoch 104/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.1954 - val_loss: 0.1746\n",
      "Epoch 105/120\n",
      "51877/51877 [==============================] - 4s 69us/sample - loss: 0.1960 - val_loss: 0.1749\n",
      "Epoch 106/120\n",
      "51877/51877 [==============================] - 3s 64us/sample - loss: 0.1968 - val_loss: 0.1736\n",
      "Epoch 107/120\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.1954 - val_loss: 0.1768\n",
      "Epoch 108/120\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 0.1948 - val_loss: 0.1738\n",
      "Epoch 109/120\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.1936 - val_loss: 0.1752\n",
      "Epoch 110/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1940 - val_loss: 0.1756\n",
      "Epoch 111/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1944 - val_loss: 0.1736\n",
      "Epoch 112/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.1937 - val_loss: 0.1746\n",
      "Epoch 113/120\n",
      "51877/51877 [==============================] - 3s 62us/sample - loss: 0.1926 - val_loss: 0.1752\n",
      "Epoch 114/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1934 - val_loss: 0.1745\n",
      "Epoch 115/120\n",
      "51877/51877 [==============================] - 2s 47us/sample - loss: 0.1953 - val_loss: 0.1748\n",
      "Epoch 116/120\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.1989 - val_loss: 0.1733\n",
      "Epoch 117/120\n",
      "51877/51877 [==============================] - 3s 67us/sample - loss: 0.1990 - val_loss: 0.1738\n",
      "Epoch 118/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1956 - val_loss: 0.1752\n",
      "Epoch 119/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1927 - val_loss: 0.1762\n",
      "Epoch 120/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.1930 - val_loss: 0.1752\n",
      "Entrenando con 128 neuronas\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/120\n",
      "51877/51877 [==============================] - 4s 68us/sample - loss: 3.5544 - val_loss: 0.8379\n",
      "Epoch 2/120\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.8993 - val_loss: 0.2920\n",
      "Epoch 3/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.4905 - val_loss: 0.2399\n",
      "Epoch 4/120\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.3482 - val_loss: 0.2110\n",
      "Epoch 5/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.3077 - val_loss: 0.2051\n",
      "Epoch 6/120\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 0.2809 - val_loss: 0.2075\n",
      "Epoch 7/120\n",
      "51877/51877 [==============================] - 3s 61us/sample - loss: 0.2651 - val_loss: 0.1985\n",
      "Epoch 8/120\n",
      "51877/51877 [==============================] - 4s 74us/sample - loss: 0.2579 - val_loss: 0.1939\n",
      "Epoch 9/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.2528 - val_loss: 0.1948\n",
      "Epoch 10/120\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.2505 - val_loss: 0.1888\n",
      "Epoch 11/120\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.2411 - val_loss: 0.1889\n",
      "Epoch 12/120\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.2425 - val_loss: 0.1950\n",
      "Epoch 13/120\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.2406 - val_loss: 0.1913\n",
      "Epoch 14/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.2318 - val_loss: 0.1853\n",
      "Epoch 15/120\n",
      "51877/51877 [==============================] - 3s 63us/sample - loss: 0.2282 - val_loss: 0.1856\n",
      "Epoch 16/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.2283 - val_loss: 0.1871\n",
      "Epoch 17/120\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.2230 - val_loss: 0.1850\n",
      "Epoch 18/120\n",
      "51877/51877 [==============================] - 4s 70us/sample - loss: 0.2225 - val_loss: 0.1860\n",
      "Epoch 19/120\n",
      "51877/51877 [==============================] - 4s 73us/sample - loss: 0.2231 - val_loss: 0.1825\n",
      "Epoch 20/120\n",
      "51877/51877 [==============================] - 4s 71us/sample - loss: 0.2185 - val_loss: 0.1811\n",
      "Epoch 21/120\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.2212 - val_loss: 0.1829\n",
      "Epoch 22/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.2168 - val_loss: 0.1805\n",
      "Epoch 23/120\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.2171 - val_loss: 0.1828\n",
      "Epoch 24/120\n",
      "51877/51877 [==============================] - 4s 70us/sample - loss: 0.2147 - val_loss: 0.1826\n",
      "Epoch 25/120\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 0.2131 - val_loss: 0.1800\n",
      "Epoch 26/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.2117 - val_loss: 0.1790\n",
      "Epoch 27/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2117 - val_loss: 0.1802\n",
      "Epoch 28/120\n",
      "51877/51877 [==============================] - 3s 65us/sample - loss: 0.2103 - val_loss: 0.1813\n",
      "Epoch 29/120\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.2107 - val_loss: 0.1847\n",
      "Epoch 30/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.2123 - val_loss: 0.1811\n",
      "Epoch 31/120\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.2106 - val_loss: 0.1828\n",
      "Epoch 32/120\n",
      "51877/51877 [==============================] - 3s 61us/sample - loss: 0.2090 - val_loss: 0.1803\n",
      "Epoch 33/120\n",
      "51877/51877 [==============================] - 3s 61us/sample - loss: 0.2150 - val_loss: 0.1825\n",
      "Epoch 34/120\n",
      "51877/51877 [==============================] - 4s 76us/sample - loss: 0.2083 - val_loss: 0.1782\n",
      "Epoch 35/120\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.2079 - val_loss: 0.1775\n",
      "Epoch 36/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2086 - val_loss: 0.1795\n",
      "Epoch 37/120\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.2071 - val_loss: 0.1784\n",
      "Epoch 38/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.2075 - val_loss: 0.1793\n",
      "Epoch 39/120\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.2043 - val_loss: 0.1810\n",
      "Epoch 40/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.2047 - val_loss: 0.1778\n",
      "Epoch 41/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.2080 - val_loss: 0.1784\n",
      "Epoch 42/120\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.2048 - val_loss: 0.1787\n",
      "Epoch 43/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.2023 - val_loss: 0.1821\n",
      "Epoch 44/120\n",
      "51877/51877 [==============================] - 3s 51us/sample - loss: 0.2034 - val_loss: 0.1771\n",
      "Epoch 45/120\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.2061 - val_loss: 0.1811\n",
      "Epoch 46/120\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.2035 - val_loss: 0.1780\n",
      "Epoch 47/120\n",
      "51877/51877 [==============================] - 4s 78us/sample - loss: 0.2038 - val_loss: 0.1773\n",
      "Epoch 48/120\n",
      "51877/51877 [==============================] - 4s 74us/sample - loss: 0.2020 - val_loss: 0.1775\n",
      "Epoch 49/120\n",
      "51877/51877 [==============================] - 4s 70us/sample - loss: 0.2016 - val_loss: 0.1801\n",
      "Epoch 50/120\n",
      "51877/51877 [==============================] - 4s 79us/sample - loss: 0.2027 - val_loss: 0.1776\n",
      "Epoch 51/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2015 - val_loss: 0.1834\n",
      "Epoch 52/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.2008 - val_loss: 0.1786\n",
      "Epoch 53/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.2007 - val_loss: 0.1780\n",
      "Epoch 54/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.2007 - val_loss: 0.1760\n",
      "Epoch 55/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.2011 - val_loss: 0.1795\n",
      "Epoch 56/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.1994 - val_loss: 0.1770\n",
      "Epoch 57/120\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.2003 - val_loss: 0.1781\n",
      "Epoch 58/120\n",
      "51877/51877 [==============================] - 3s 67us/sample - loss: 0.1990 - val_loss: 0.1780\n",
      "Epoch 59/120\n",
      "51877/51877 [==============================] - 3s 67us/sample - loss: 0.1989 - val_loss: 0.1776\n",
      "Epoch 60/120\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.1993 - val_loss: 0.1782\n",
      "Epoch 61/120\n",
      "51877/51877 [==============================] - 3s 64us/sample - loss: 0.1996 - val_loss: 0.1771\n",
      "Epoch 62/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.1982 - val_loss: 0.1767\n",
      "Epoch 63/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.1986 - val_loss: 0.1747\n",
      "Epoch 64/120\n",
      "51877/51877 [==============================] - 3s 64us/sample - loss: 0.1980 - val_loss: 0.1770\n",
      "Epoch 65/120\n",
      "51877/51877 [==============================] - 3s 66us/sample - loss: 0.1992 - val_loss: 0.1773\n",
      "Epoch 66/120\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 0.1991 - val_loss: 0.1763\n",
      "Epoch 67/120\n",
      "51877/51877 [==============================] - 3s 64us/sample - loss: 0.1985 - val_loss: 0.1783\n",
      "Epoch 68/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.1961 - val_loss: 0.1748\n",
      "Epoch 69/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.1972 - val_loss: 0.1743\n",
      "Epoch 70/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.1956 - val_loss: 0.1767\n",
      "Epoch 71/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.1979 - val_loss: 0.1770\n",
      "Epoch 72/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.1960 - val_loss: 0.1770\n",
      "Epoch 73/120\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.1953 - val_loss: 0.1745\n",
      "Epoch 74/120\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.1959 - val_loss: 0.1768\n",
      "Epoch 75/120\n",
      "51877/51877 [==============================] - 4s 68us/sample - loss: 0.1962 - val_loss: 0.1776\n",
      "Epoch 76/120\n",
      "51877/51877 [==============================] - 3s 66us/sample - loss: 0.1961 - val_loss: 0.1755\n",
      "Epoch 77/120\n",
      "51877/51877 [==============================] - 3s 64us/sample - loss: 0.1960 - val_loss: 0.1754\n",
      "Epoch 78/120\n",
      "51877/51877 [==============================] - 3s 64us/sample - loss: 0.1955 - val_loss: 0.1741\n",
      "Epoch 79/120\n",
      "51877/51877 [==============================] - 3s 62us/sample - loss: 0.1951 - val_loss: 0.1758\n",
      "Epoch 80/120\n",
      "51877/51877 [==============================] - 4s 70us/sample - loss: 0.1952 - val_loss: 0.1753\n",
      "Epoch 81/120\n",
      "51877/51877 [==============================] - 3s 61us/sample - loss: 0.1955 - val_loss: 0.1754\n",
      "Epoch 82/120\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.1946 - val_loss: 0.1759\n",
      "Epoch 83/120\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.1941 - val_loss: 0.1739\n",
      "Epoch 84/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.1949 - val_loss: 0.1747\n",
      "Epoch 85/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.1941 - val_loss: 0.1777\n",
      "Epoch 86/120\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.1957 - val_loss: 0.1746\n",
      "Epoch 87/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.1965 - val_loss: 0.1771\n",
      "Epoch 88/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.1942 - val_loss: 0.1763\n",
      "Epoch 89/120\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 0.1943 - val_loss: 0.1751\n",
      "Epoch 90/120\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.1937 - val_loss: 0.1745\n",
      "Epoch 91/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.1949 - val_loss: 0.1743\n",
      "Epoch 92/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.1930 - val_loss: 0.1753\n",
      "Epoch 93/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.1945 - val_loss: 0.1767\n",
      "Epoch 94/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.1926 - val_loss: 0.1741\n",
      "Epoch 95/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.1930 - val_loss: 0.1754\n",
      "Epoch 96/120\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.1935 - val_loss: 0.1786\n",
      "Epoch 97/120\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.1925 - val_loss: 0.1743\n",
      "Epoch 98/120\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.1942 - val_loss: 0.1753\n",
      "Epoch 99/120\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.1932 - val_loss: 0.1742\n",
      "Epoch 100/120\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 0.1929 - val_loss: 0.1754\n",
      "Epoch 101/120\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.1938 - val_loss: 0.1755\n",
      "Epoch 102/120\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.1922 - val_loss: 0.1754\n",
      "Epoch 103/120\n",
      "51877/51877 [==============================] - 3s 52us/sample - loss: 0.1929 - val_loss: 0.1759\n",
      "Epoch 104/120\n",
      "51877/51877 [==============================] - 4s 68us/sample - loss: 0.1933 - val_loss: 0.1751\n",
      "Epoch 105/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.1922 - val_loss: 0.1750\n",
      "Epoch 106/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1910 - val_loss: 0.1730\n",
      "Epoch 107/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1928 - val_loss: 0.1755\n",
      "Epoch 108/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1905 - val_loss: 0.1749\n",
      "Epoch 109/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1918 - val_loss: 0.1754\n",
      "Epoch 110/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.1909 - val_loss: 0.1739\n",
      "Epoch 111/120\n",
      "51877/51877 [==============================] - 3s 54us/sample - loss: 0.1908 - val_loss: 0.1736\n",
      "Epoch 112/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1916 - val_loss: 0.1746\n",
      "Epoch 113/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1924 - val_loss: 0.1746\n",
      "Epoch 114/120\n",
      "51877/51877 [==============================] - 3s 53us/sample - loss: 0.1907 - val_loss: 0.1739\n",
      "Epoch 115/120\n",
      "51877/51877 [==============================] - 3s 65us/sample - loss: 0.1917 - val_loss: 0.1743\n",
      "Epoch 116/120\n",
      "51877/51877 [==============================] - 3s 67us/sample - loss: 0.1913 - val_loss: 0.1745\n",
      "Epoch 117/120\n",
      "51877/51877 [==============================] - 4s 68us/sample - loss: 0.1908 - val_loss: 0.1751\n",
      "Epoch 118/120\n",
      "51877/51877 [==============================] - 4s 69us/sample - loss: 0.1915 - val_loss: 0.1772\n",
      "Epoch 119/120\n",
      "51877/51877 [==============================] - 4s 73us/sample - loss: 0.1917 - val_loss: 0.1769\n",
      "Epoch 120/120\n",
      "51877/51877 [==============================] - 3s 66us/sample - loss: 0.1904 - val_loss: 0.1747\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "print('Entrenando con 64 neuronas')\n",
    "inputs = Input(shape=(x_train.shape[1],))\n",
    "hidden1 = Dense(64)(inputs)\n",
    "activ1 = LeakyReLU(0.5)(hidden1)\n",
    "drop1 = Dropout(0.5)(activ1)\n",
    "outputs = Dense(1)(drop1)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=SGD(lr=0.001), loss='mse')\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=120, validation_data=(x_val, y_val), callbacks=[terminate])\n",
    "\n",
    "print('Entrenando con 128 neuronas')\n",
    "inputs = Input(shape=(x_train.shape[1],))\n",
    "hidden1 = Dense(128)(inputs)\n",
    "activ1 = LeakyReLU(0.5)(hidden1)\n",
    "drop1 = Dropout(0.5)(activ1)\n",
    "outputs = Dense(1)(drop1)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=SGD(lr=0.001), loss='mse')\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=120, validation_data=(x_val, y_val), callbacks=[terminate])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x3-T18tkG-lC"
   },
   "source": [
    "### 1.i Extreme Learning Machine\n",
    "\n",
    "Otra aproximación para obtener modelos grandes que no sobreajustan es la implementada por _ELM_. Explique en qué consiste la idea de _ELM_ y porqué esto podría evitar sobreajuste a pesar de utilizar modelos con gran número de parámetros. \n",
    "\n",
    "Entrene una _ELM_ de una capa fija y una capa oculta, la primera con un número relativamente grande y la segunda con un número relativamente pequeño. Puede utilizar los valores propuestos en el código u otros que le parezcan convenientes. \n",
    "\n",
    "Comente sobre el número total de parámetros y el número de parametros entrenables con respecto a los modelos anteriores. ¿Cómo se desempeña la red? ¿El número elevado de parámetros totales implica necesariamente _overfitting_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QdBa4MYFG-lD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/20\n",
      "51877/51877 [==============================] - 4s 81us/sample - loss: 2.1622 - val_loss: 0.5913\n",
      "Epoch 2/20\n",
      "51877/51877 [==============================] - 3s 61us/sample - loss: 0.3232 - val_loss: 0.2610\n",
      "Epoch 3/20\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 0.2199 - val_loss: 0.2216\n",
      "Epoch 4/20\n",
      "51877/51877 [==============================] - 3s 63us/sample - loss: 0.2030 - val_loss: 0.2019\n",
      "Epoch 5/20\n",
      "51877/51877 [==============================] - 3s 67us/sample - loss: 0.1965 - val_loss: 0.1991\n",
      "Epoch 6/20\n",
      "51877/51877 [==============================] - 3s 62us/sample - loss: 0.1920 - val_loss: 0.2216\n",
      "Epoch 7/20\n",
      "51877/51877 [==============================] - 4s 74us/sample - loss: 0.1881 - val_loss: 0.1993\n",
      "Epoch 8/20\n",
      "51877/51877 [==============================] - 3s 62us/sample - loss: 0.1853 - val_loss: 0.1926\n",
      "Epoch 9/20\n",
      "51877/51877 [==============================] - 4s 69us/sample - loss: 0.1826 - val_loss: 0.2035\n",
      "Epoch 10/20\n",
      "51877/51877 [==============================] - 4s 73us/sample - loss: 0.1805 - val_loss: 0.1924\n",
      "Epoch 11/20\n",
      "51877/51877 [==============================] - 3s 67us/sample - loss: 0.1783 - val_loss: 0.1892\n",
      "Epoch 12/20\n",
      "51877/51877 [==============================] - 4s 76us/sample - loss: 0.1766 - val_loss: 0.1899\n",
      "Epoch 13/20\n",
      "51877/51877 [==============================] - 4s 85us/sample - loss: 0.1748 - val_loss: 0.1871\n",
      "Epoch 14/20\n",
      "51877/51877 [==============================] - 4s 68us/sample - loss: 0.1731 - val_loss: 0.1848\n",
      "Epoch 15/20\n",
      "51877/51877 [==============================] - 3s 63us/sample - loss: 0.1717 - val_loss: 0.1841\n",
      "Epoch 16/20\n",
      "51877/51877 [==============================] - 3s 63us/sample - loss: 0.1701 - val_loss: 0.1974\n",
      "Epoch 17/20\n",
      "51877/51877 [==============================] - 3s 62us/sample - loss: 0.1688 - val_loss: 0.1833\n",
      "Epoch 18/20\n",
      "51877/51877 [==============================] - 4s 69us/sample - loss: 0.1677 - val_loss: 0.1824\n",
      "Epoch 19/20\n",
      "51877/51877 [==============================] - 3s 64us/sample - loss: 0.1666 - val_loss: 0.1856\n",
      "Epoch 20/20\n",
      "51877/51877 [==============================] - 4s 73us/sample - loss: 0.1655 - val_loss: 0.1851\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(x_train.shape[1],))\n",
    "hidden1 = Dense(5000, activation='relu', name='extreme')(inputs)\n",
    "hidden2 = Dense(32, activation='relu')(hidden1)\n",
    "outputs = Dense(1)(hidden2)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.get_layer('extreme').trainable = False\n",
    "\n",
    "model.compile(optimizer=SGD(lr=0.001), loss='mse')\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val), callbacks=[terminate])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72-JP1ieG-lH"
   },
   "source": [
    "### 1.j Learning Rate Decay\n",
    "Ahora entrenaremos un modelo manejando manualmente el _learn rate_. Para esto utilizaremos el _callback_ `LearningRateScheduler`. Este _callback_ nos permitirá implementar una función que maneje el _learn rate_ de nuestro modelo. \n",
    "\n",
    "Escriba una función que reciba la epoca actual y retorne un _learn rate_ lr. El lr inicial debe ser igual o mayor a alguno que haya dado buenos resultados en las preguntas anteriores. La función debe dividir por 2 el lr cada 10 _epochs_. Además ponga como restricción que el lr no debe ser menor a $5\\times 10^{-5}$, es decir si el valor obtenido es menor a  $5\\times 10^{-5}$, la función retorna  $5\\times 10^{-5}$.\n",
    "\n",
    "Entrene su red preferida de las preguntas anteriores con esta modificación, grafique los errores a lo largo del entrenamiento y comente. Según lo visto en el ramo, ¿por qué podría ser util disminuir el _learn rate_ a medida se avanza en el aprendizaje de la red?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0MOn5W6LG-lI",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.0005\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.00025\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "0.000125\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "6.25e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "5e-05\n",
      "Entrenando con 128 neuronas\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/60\n",
      "51877/51877 [==============================] - 4s 82us/sample - loss: 3.5983 - val_loss: 1.0618\n",
      "Epoch 2/60\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 1.0131 - val_loss: 0.4456\n",
      "Epoch 3/60\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.4675 - val_loss: 0.2607\n",
      "Epoch 4/60\n",
      "51877/51877 [==============================] - 4s 69us/sample - loss: 0.3486 - val_loss: 0.2313\n",
      "Epoch 5/60\n",
      "51877/51877 [==============================] - 4s 75us/sample - loss: 0.3158 - val_loss: 0.2103\n",
      "Epoch 6/60\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.2825 - val_loss: 0.2136\n",
      "Epoch 7/60\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.2710 - val_loss: 0.2021\n",
      "Epoch 8/60\n",
      "51877/51877 [==============================] - 4s 73us/sample - loss: 0.2741 - val_loss: 0.1974\n",
      "Epoch 9/60\n",
      "51877/51877 [==============================] - 3s 61us/sample - loss: 0.2558 - val_loss: 0.1983\n",
      "Epoch 10/60\n",
      "51877/51877 [==============================] - 3s 67us/sample - loss: 0.2466 - val_loss: 0.1988\n",
      "Epoch 11/60\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 0.2402 - val_loss: 0.1922\n",
      "Epoch 12/60\n",
      "51877/51877 [==============================] - 3s 64us/sample - loss: 0.2359 - val_loss: 0.1913\n",
      "Epoch 13/60\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 0.2357 - val_loss: 0.1904\n",
      "Epoch 14/60\n",
      "51877/51877 [==============================] - 3s 66us/sample - loss: 0.2343 - val_loss: 0.1913\n",
      "Epoch 15/60\n",
      "51877/51877 [==============================] - 4s 81us/sample - loss: 0.2338 - val_loss: 0.1926\n",
      "Epoch 16/60\n",
      "51877/51877 [==============================] - 3s 66us/sample - loss: 0.2319 - val_loss: 0.1904\n",
      "Epoch 17/60\n",
      "51877/51877 [==============================] - 3s 64us/sample - loss: 0.2305 - val_loss: 0.1864\n",
      "Epoch 18/60\n",
      "51877/51877 [==============================] - 3s 65us/sample - loss: 0.2263 - val_loss: 0.1901\n",
      "Epoch 19/60\n",
      "51877/51877 [==============================] - 3s 61us/sample - loss: 0.2266 - val_loss: 0.1900\n",
      "Epoch 20/60\n",
      "51877/51877 [==============================] - 4s 75us/sample - loss: 0.2259 - val_loss: 0.1843\n",
      "Epoch 21/60\n",
      "51877/51877 [==============================] - 3s 61us/sample - loss: 0.2236 - val_loss: 0.1869\n",
      "Epoch 22/60\n",
      "51877/51877 [==============================] - 3s 67us/sample - loss: 0.2226 - val_loss: 0.1873\n",
      "Epoch 23/60\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.2223 - val_loss: 0.1854\n",
      "Epoch 24/60\n",
      "51877/51877 [==============================] - 3s 60us/sample - loss: 0.2227 - val_loss: 0.1867\n",
      "Epoch 25/60\n",
      "51877/51877 [==============================] - 3s 64us/sample - loss: 0.2200 - val_loss: 0.1848\n",
      "Epoch 26/60\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.2227 - val_loss: 0.1847\n",
      "Epoch 27/60\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.2192 - val_loss: 0.1854\n",
      "Epoch 28/60\n",
      "51877/51877 [==============================] - 4s 82us/sample - loss: 0.2195 - val_loss: 0.1873\n",
      "Epoch 29/60\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.2164 - val_loss: 0.1848\n",
      "Epoch 30/60\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.2211 - val_loss: 0.1840\n",
      "Epoch 31/60\n",
      "51877/51877 [==============================] - 5s 92us/sample - loss: 0.2185 - val_loss: 0.1835\n",
      "Epoch 32/60\n",
      "51877/51877 [==============================] - 4s 73us/sample - loss: 0.2172 - val_loss: 0.1836\n",
      "Epoch 33/60\n",
      "51877/51877 [==============================] - 4s 73us/sample - loss: 0.2183 - val_loss: 0.1832\n",
      "Epoch 34/60\n",
      "51877/51877 [==============================] - 3s 64us/sample - loss: 0.2178 - val_loss: 0.1824\n",
      "Epoch 35/60\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.2180 - val_loss: 0.1824\n",
      "Epoch 36/60\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.2172 - val_loss: 0.1827\n",
      "Epoch 37/60\n",
      "51877/51877 [==============================] - 3s 55us/sample - loss: 0.2149 - val_loss: 0.1819\n",
      "Epoch 38/60\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.2162 - val_loss: 0.1834\n",
      "Epoch 39/60\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.2163 - val_loss: 0.1823\n",
      "Epoch 40/60\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.2189 - val_loss: 0.1824\n",
      "Epoch 41/60\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.2165 - val_loss: 0.1821\n",
      "Epoch 42/60\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.2159 - val_loss: 0.1817\n",
      "Epoch 43/60\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.2153 - val_loss: 0.1822\n",
      "Epoch 44/60\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.2153 - val_loss: 0.1823\n",
      "Epoch 45/60\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.2149 - val_loss: 0.1830\n",
      "Epoch 46/60\n",
      "51877/51877 [==============================] - 3s 59us/sample - loss: 0.2159 - val_loss: 0.1830\n",
      "Epoch 47/60\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.2158 - val_loss: 0.1825\n",
      "Epoch 48/60\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.2159 - val_loss: 0.1828\n",
      "Epoch 49/60\n",
      "51877/51877 [==============================] - 4s 76us/sample - loss: 0.2161 - val_loss: 0.1824\n",
      "Epoch 50/60\n",
      "51877/51877 [==============================] - 3s 64us/sample - loss: 0.2159 - val_loss: 0.1820\n",
      "Epoch 51/60\n",
      "51877/51877 [==============================] - 3s 56us/sample - loss: 0.2146 - val_loss: 0.1823\n",
      "Epoch 52/60\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.2136 - val_loss: 0.1818\n",
      "Epoch 53/60\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.2160 - val_loss: 0.1823\n",
      "Epoch 54/60\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.2148 - val_loss: 0.1823\n",
      "Epoch 55/60\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.2148 - val_loss: 0.1822\n",
      "Epoch 56/60\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.2149 - val_loss: 0.1819\n",
      "Epoch 57/60\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.2141 - val_loss: 0.1822\n",
      "Epoch 58/60\n",
      "51877/51877 [==============================] - 3s 58us/sample - loss: 0.2167 - val_loss: 0.1824\n",
      "Epoch 59/60\n",
      "51877/51877 [==============================] - 3s 57us/sample - loss: 0.2143 - val_loss: 0.1820\n",
      "Epoch 60/60\n",
      "51877/51877 [==============================] - 3s 62us/sample - loss: 0.2140 - val_loss: 0.1815\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def step_decay(epoch):\n",
    "    lr = 0.001 * 0.5**(epoch//10)\n",
    "    if lr <= 5*10**-5:\n",
    "        return 5*10**-5\n",
    "    return lr\n",
    "\n",
    "schedule = LearningRateScheduler(step_decay)\n",
    "\n",
    "print('Entrenando con 128 neuronas')\n",
    "inputs = Input(shape=(x_train.shape[1],))\n",
    "hidden1 = Dense(128)(inputs)\n",
    "activ1 = LeakyReLU(0.5)(hidden1)\n",
    "drop1 = Dropout(0.5)(activ1)\n",
    "outputs = Dense(1)(drop1)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=SGD(lr=0.001), loss='mse')\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=60, validation_data=(x_val, y_val), callbacks=[schedule, terminate])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bHdXHEiXG-lN"
   },
   "source": [
    "### 1.k Vanishing Gradient\n",
    "\n",
    "El fenómeno del _vanishing gradient_ es el rápido decaimiento del paso de _Backpropagation_ al avanzar por las capas. A lo largo de la tarea solo hemos entrenado capas con una red oculta, de igual forma que la comunidad cientifica realizo por largo tiempo, por el problema del _vanishing gradient_ y por el teorema de aproximación universal que resumidamente demuestra que una red de una sola capa puede aproximar una amplia familia de funciones. \n",
    "\n",
    "En esta pregunta entrenaremos una red neuronal profunda sin implementar ninguno de los dispositivos que permiten hoy en día sortear el problema del _vanishing gradient_, para ponerlo en evidencia. Para esto construya una red con 6 capas ocultas, con la siguiente lista de numero de neuronas: $256$ $256$ $128$ $128$ $32$ y $32$, o con valores similares. De tal manera obtendrá un valor de parámetros relativamente comparable a los valores utilizados en las primeras redes. \n",
    "\n",
    "Grafique un histograma con los pesos de las 6 capas densas de la red sin entrenar, entrenela a completitud con el método que estime conveniente y luego grafique nuevamente los histogramas para las 6 capas. Comente lo que observa. \n",
    "\n",
    "Luego, pruebe cambiar la inizialización de los pesos de la capa densa, puede revisar la documentación de keras para ver las opciones existentes a parte de `glorot_uniform` por defecto. ¿Se logra solucionar el problema? \n",
    "\n",
    "Por último, pruebe aumentar la tasa de aprendizaje para ver si logra hacer que el paso de _backpropagation_ alcance las capas que anteriormente no se entrenaban. ¿Qué observa en este caso? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NmcoqutlG-lP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/10\n",
      "51877/51877 [==============================] - 8s 153us/sample - loss: 1.0809 - val_loss: 0.2567\n",
      "Epoch 2/10\n",
      "51877/51877 [==============================] - 4s 83us/sample - loss: 0.2245 - val_loss: 0.2221\n",
      "Epoch 3/10\n",
      "51877/51877 [==============================] - 5s 92us/sample - loss: 0.2014 - val_loss: 0.2141\n",
      "Epoch 4/10\n",
      "51877/51877 [==============================] - 5s 89us/sample - loss: 0.1901 - val_loss: 0.2054\n",
      "Epoch 5/10\n",
      "51877/51877 [==============================] - 4s 85us/sample - loss: 0.1825 - val_loss: 0.1986\n",
      "Epoch 6/10\n",
      "51877/51877 [==============================] - 4s 84us/sample - loss: 0.1768 - val_loss: 0.1957\n",
      "Epoch 7/10\n",
      "51877/51877 [==============================] - 4s 83us/sample - loss: 0.1721 - val_loss: 0.1958\n",
      "Epoch 8/10\n",
      "51877/51877 [==============================] - 4s 82us/sample - loss: 0.1680 - val_loss: 0.2026\n",
      "Epoch 9/10\n",
      "51877/51877 [==============================] - 4s 83us/sample - loss: 0.1648 - val_loss: 0.1906\n",
      "Epoch 10/10\n",
      "51877/51877 [==============================] - 4s 84us/sample - loss: 0.1616 - val_loss: 0.1888\n"
     ]
    }
   ],
   "source": [
    "# Red de 6 capas\n",
    "inputs = Input(shape=(x_train.shape[1],))\n",
    "hidden1 = Dense(256, activation='relu', name='hidden1')(inputs)\n",
    "hidden2 = Dense(256, activation='relu', name='hidden2')(hidden1)\n",
    "hidden3 = Dense(128, activation='relu', name='hidden3')(hidden2)\n",
    "hidden4 = Dense(128, activation='relu', name='hidden4')(hidden3)\n",
    "hidden5 = Dense(32, activation='relu', name='hidden5')(hidden4)\n",
    "hidden6 = Dense(32, activation='relu', name='hidden6')(hidden5)\n",
    "outputs = Dense(1)(hidden6)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=SGD(lr=0.001), loss='mse')\n",
    "\n",
    "\n",
    "weights1 = []\n",
    "for name in ['hidden'+str(i) for i in range(1,7)]:\n",
    "    weights1.append(model.get_layer(name).get_weights()[0].flatten())\n",
    "    weights1.append(model.get_layer(name).get_weights()[1])\n",
    "    \n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val), callbacks=[terminate])\n",
    "\n",
    "weights2 = []\n",
    "for name in ['hidden'+str(i) for i in range(1,7)]:\n",
    "    weights2.append(model.get_layer(name).get_weights()[0].flatten())\n",
    "    weights2.append(model.get_layer(name).get_weights()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NmcoqutlG-lP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/10\n",
      "51877/51877 [==============================] - 7s 136us/sample - loss: 1.4098 - val_loss: 0.3205\n",
      "Epoch 2/10\n",
      "51877/51877 [==============================] - 4s 83us/sample - loss: 0.2614 - val_loss: 0.3036\n",
      "Epoch 3/10\n",
      "51877/51877 [==============================] - 4s 83us/sample - loss: 0.2194 - val_loss: 0.2250\n",
      "Epoch 4/10\n",
      "51877/51877 [==============================] - 4s 81us/sample - loss: 0.1994 - val_loss: 0.2149\n",
      "Epoch 5/10\n",
      "51877/51877 [==============================] - 4s 81us/sample - loss: 0.1881 - val_loss: 0.2102\n",
      "Epoch 6/10\n",
      "51877/51877 [==============================] - 4s 82us/sample - loss: 0.1801 - val_loss: 0.2102\n",
      "Epoch 7/10\n",
      "51877/51877 [==============================] - 4s 82us/sample - loss: 0.1739 - val_loss: 0.2037\n",
      "Epoch 8/10\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.1688 - val_loss: 0.2020\n",
      "Epoch 9/10\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.1646 - val_loss: 0.1990\n",
      "Epoch 10/10\n",
      "51877/51877 [==============================] - 4s 82us/sample - loss: 0.1607 - val_loss: 0.2001\n"
     ]
    }
   ],
   "source": [
    "# Red de 6 capas con inicialización he_uniform\n",
    "inputs = Input(shape=(x_train.shape[1],))\n",
    "hidden1 = Dense(256, activation='relu', name='hidden1', kernel_initializer='he_uniform')(inputs)\n",
    "hidden2 = Dense(256, activation='relu', name='hidden2', kernel_initializer='he_uniform')(hidden1)\n",
    "hidden3 = Dense(128, activation='relu', name='hidden3', kernel_initializer='he_uniform')(hidden2)\n",
    "hidden4 = Dense(128, activation='relu', name='hidden4', kernel_initializer='he_uniform')(hidden3)\n",
    "hidden5 = Dense(32, activation='relu', name='hidden5', kernel_initializer='he_uniform')(hidden4)\n",
    "hidden6 = Dense(32, activation='relu', name='hidden6', kernel_initializer='he_uniform')(hidden5)\n",
    "outputs = Dense(1)(hidden6)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=SGD(lr=0.001), loss='mse')\n",
    "\n",
    "\n",
    "weights3 = []\n",
    "for name in ['hidden'+str(i) for i in range(1,7)]:\n",
    "    weights3.append(model.get_layer(name).get_weights()[0].flatten())\n",
    "    weights3.append(model.get_layer(name).get_weights()[1])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val), callbacks=[terminate])\n",
    "\n",
    "weights4 = []\n",
    "for name in ['hidden'+str(i) for i in range(1,7)]:\n",
    "    weights4.append(model.get_layer(name).get_weights()[0].flatten())\n",
    "    weights4.append(model.get_layer(name).get_weights()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NmcoqutlG-lP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/10\n",
      "51877/51877 [==============================] - 7s 138us/sample - loss: 0.4920 - val_loss: 0.1952\n",
      "Epoch 2/10\n",
      "51877/51877 [==============================] - 4s 82us/sample - loss: 0.1873 - val_loss: 0.1827\n",
      "Epoch 3/10\n",
      "51877/51877 [==============================] - 4s 83us/sample - loss: 0.1719 - val_loss: 0.1852\n",
      "Epoch 4/10\n",
      "51877/51877 [==============================] - 5s 88us/sample - loss: 0.1634 - val_loss: 0.1803\n",
      "Epoch 5/10\n",
      "51877/51877 [==============================] - 4s 83us/sample - loss: 0.1566 - val_loss: 0.1790\n",
      "Epoch 6/10\n",
      "51877/51877 [==============================] - 4s 82us/sample - loss: 0.1507 - val_loss: 0.1865\n",
      "Epoch 7/10\n",
      "51877/51877 [==============================] - 4s 83us/sample - loss: 0.1453 - val_loss: 0.1768\n",
      "Epoch 8/10\n",
      "51877/51877 [==============================] - 4s 82us/sample - loss: 0.1407 - val_loss: 0.1769\n",
      "Epoch 9/10\n",
      "51877/51877 [==============================] - 4s 81us/sample - loss: 0.1364 - val_loss: 0.1753\n",
      "Epoch 10/10\n",
      "51877/51877 [==============================] - 4s 82us/sample - loss: 0.1322 - val_loss: 0.2013\n"
     ]
    }
   ],
   "source": [
    "# Red de 6 capas con mayor learning rate\n",
    "inputs = Input(shape=(x_train.shape[1],))\n",
    "hidden1 = Dense(256, activation='relu', name='hidden1')(inputs)\n",
    "hidden2 = Dense(256, activation='relu', name='hidden2')(hidden1)\n",
    "hidden3 = Dense(128, activation='relu', name='hidden3')(hidden2)\n",
    "hidden4 = Dense(128, activation='relu', name='hidden4')(hidden3)\n",
    "hidden5 = Dense(32, activation='relu', name='hidden5')(hidden4)\n",
    "hidden6 = Dense(32, activation='relu', name='hidden6')(hidden5)\n",
    "outputs = Dense(1)(hidden6)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=SGD(lr=0.005), loss='mse')\n",
    "\n",
    "\n",
    "weights5 = []\n",
    "for name in ['hidden'+str(i) for i in range(1,7)]:\n",
    "    weights5.append(model.get_layer(name).get_weights()[0].flatten())\n",
    "    weights5.append(model.get_layer(name).get_weights()[1])\n",
    "    \n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val), callbacks=[terminate])\n",
    "\n",
    "weights6 = []\n",
    "for name in ['hidden'+str(i) for i in range(1,7)]:\n",
    "    weights6.append(model.get_layer(name).get_weights()[0].flatten())\n",
    "    weights6.append(model.get_layer(name).get_weights()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax1 = plt.subplot(3, 2, 1)\n",
    "plt.hist(np.concatenate(weights1).flatten(), bins=60)\n",
    "plt.subplot(3, 2, 2, sharex=ax1, sharey=ax1)\n",
    "plt.hist(np.concatenate(weights2).flatten(), bins=60)\n",
    "plt.subplot(3, 2, 3, sharex=ax1, sharey=ax1)\n",
    "plt.hist(np.concatenate(weights3).flatten(), bins=60)\n",
    "plt.subplot(3, 2, 4, sharex=ax1, sharey=ax1)\n",
    "plt.hist(np.concatenate(weights4).flatten(), bins=60)\n",
    "plt.subplot(3, 2, 5, sharex=ax1, sharey=ax1)\n",
    "plt.hist(np.concatenate(weights5).flatten(), bins=60)\n",
    "plt.subplot(3, 2, 6, sharex=ax1, sharey=ax1)\n",
    "plt.hist(np.concatenate(weights6).flatten(), bins=60)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhstdi4cG-lY"
   },
   "source": [
    "### 1.l Otros Optimizadores\n",
    "\n",
    "Finalmente, utilizando la estructura de red que mejor se haya desempeñado a lo largo de la tarea, entrene esta red utilizando un optimizador distinto al gradiente descendente vainilla. Pruebe al menos 2 optimizadores implementados en keras (puede utilizar Adam, AdaGrad, AdaDelta, RMSprop, entre otros) o modificar los parámetros que no hemos utilizado del gradiente descendente (momentum, momentum de Nesterov...).\n",
    "\n",
    "Note que por las inicializaciones por defecto de los pesos de las capas y la naturaleza de los datos en cuestión, puede ocurrir que para los valores defecto de algunos optimizadores la red diverga en las primeras iteraciones. Para fijar los parámeros de los optimizadores debe importarlos desde `keras.optimizers` y pasar el objeto con los parámetros deseados al método `.compile` de su modelo. En cambio si con los valores usuales basta, algunos optimizadores pueden pasarse como `string` a `.compile`.\n",
    "\n",
    "Compare como se desempeñan estos optimizadores con la versión utilizada anteriormente, considerando los tiempos de entrenamiento, la velocidad de convergencia y el desempeño final alcanzado. Apoyese de gráficos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MGh3gg7jG-la",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/60\n",
      "51877/51877 [==============================] - 7s 137us/sample - loss: 21.4516 - val_loss: 8.1287\n",
      "Epoch 2/60\n",
      "51877/51877 [==============================] - 4s 78us/sample - loss: 7.0028 - val_loss: 2.8258\n",
      "Epoch 3/60\n",
      "51877/51877 [==============================] - 4s 77us/sample - loss: 2.3471 - val_loss: 0.8556\n",
      "Epoch 4/60\n",
      "51877/51877 [==============================] - 4s 84us/sample - loss: 0.8971 - val_loss: 0.6881\n",
      "Epoch 5/60\n",
      "51877/51877 [==============================] - 4s 77us/sample - loss: 0.7536 - val_loss: 0.5599\n",
      "Epoch 6/60\n",
      "51877/51877 [==============================] - 4s 86us/sample - loss: 0.6442 - val_loss: 0.3772\n",
      "Epoch 7/60\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.6691 - val_loss: 0.7096\n",
      "Epoch 8/60\n",
      "51877/51877 [==============================] - 4s 79us/sample - loss: 0.6423 - val_loss: 0.6031\n",
      "Epoch 9/60\n",
      "51877/51877 [==============================] - 4s 79us/sample - loss: 0.9377 - val_loss: 0.4935\n",
      "Epoch 10/60\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.7285 - val_loss: 0.4473\n",
      "Epoch 11/60\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.9181 - val_loss: 0.7781\n",
      "Epoch 12/60\n",
      "51877/51877 [==============================] - 4s 79us/sample - loss: 0.7405 - val_loss: 1.6128\n",
      "Epoch 13/60\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.7967 - val_loss: 0.3496\n",
      "Epoch 14/60\n",
      "51877/51877 [==============================] - 4s 81us/sample - loss: 0.6411 - val_loss: 0.8218\n",
      "Epoch 15/60\n",
      "51877/51877 [==============================] - 4s 81us/sample - loss: 0.9186 - val_loss: 0.4196\n",
      "Epoch 16/60\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.7884 - val_loss: 0.6220\n",
      "Epoch 17/60\n",
      "51877/51877 [==============================] - 4s 81us/sample - loss: 0.8147 - val_loss: 0.6854\n",
      "Epoch 18/60\n",
      "51877/51877 [==============================] - 6s 107us/sample - loss: 0.8957 - val_loss: 0.9429\n",
      "Epoch 19/60\n",
      "51877/51877 [==============================] - 4s 81us/sample - loss: 0.7841 - val_loss: 0.3388\n",
      "Epoch 20/60\n",
      "51877/51877 [==============================] - 4s 84us/sample - loss: 0.7186 - val_loss: 0.5042\n",
      "Epoch 21/60\n",
      "51877/51877 [==============================] - 4s 79us/sample - loss: 0.9107 - val_loss: 0.5335\n",
      "Epoch 22/60\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.6193 - val_loss: 0.5409\n",
      "Epoch 23/60\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.9903 - val_loss: 0.9502\n",
      "Epoch 24/60\n",
      "51877/51877 [==============================] - 4s 78us/sample - loss: 0.7812 - val_loss: 0.3536\n",
      "Epoch 25/60\n",
      "51877/51877 [==============================] - 4s 78us/sample - loss: 0.6987 - val_loss: 0.7392\n",
      "Epoch 26/60\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.9135 - val_loss: 0.4654\n",
      "Epoch 27/60\n",
      "51877/51877 [==============================] - 4s 81us/sample - loss: 0.6106 - val_loss: 0.6949\n",
      "Epoch 28/60\n",
      "51877/51877 [==============================] - 4s 79us/sample - loss: 0.9267 - val_loss: 0.8169\n",
      "Epoch 29/60\n",
      "51877/51877 [==============================] - 4s 79us/sample - loss: 0.8714 - val_loss: 0.7559\n",
      "Epoch 30/60\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.7257 - val_loss: 0.5296\n",
      "Epoch 31/60\n",
      "51877/51877 [==============================] - 4s 81us/sample - loss: 0.5690 - val_loss: 0.5716\n",
      "Epoch 32/60\n",
      "51877/51877 [==============================] - 4s 84us/sample - loss: 1.0140 - val_loss: 0.5479\n",
      "Epoch 33/60\n",
      "51877/51877 [==============================] - 4s 84us/sample - loss: 0.6706 - val_loss: 0.5633\n",
      "Epoch 34/60\n",
      "51877/51877 [==============================] - 4s 82us/sample - loss: 0.8676 - val_loss: 0.6058\n",
      "Epoch 35/60\n",
      "51877/51877 [==============================] - 5s 89us/sample - loss: 0.5813 - val_loss: 0.4180\n",
      "Epoch 36/60\n",
      "51877/51877 [==============================] - 4s 79us/sample - loss: 0.7282 - val_loss: 0.4798\n",
      "Epoch 37/60\n",
      "51877/51877 [==============================] - 4s 79us/sample - loss: 0.7658 - val_loss: 1.3931\n",
      "Epoch 38/60\n",
      "51877/51877 [==============================] - 4s 79us/sample - loss: 0.8311 - val_loss: 0.7071\n",
      "Epoch 39/60\n",
      "51877/51877 [==============================] - 5s 88us/sample - loss: 0.7407 - val_loss: 0.5148\n",
      "Epoch 40/60\n",
      "51877/51877 [==============================] - 5s 93us/sample - loss: 0.8448 - val_loss: 0.6846\n",
      "Epoch 41/60\n",
      "51877/51877 [==============================] - 5s 88us/sample - loss: 0.5955 - val_loss: 0.8278\n",
      "Epoch 42/60\n",
      "51877/51877 [==============================] - 4s 83us/sample - loss: 0.8648 - val_loss: 0.8163\n",
      "Epoch 43/60\n",
      "51877/51877 [==============================] - 4s 82us/sample - loss: 1.1693 - val_loss: 0.5093\n",
      "Epoch 44/60\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.5260 - val_loss: 0.2686\n",
      "Epoch 45/60\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.7776 - val_loss: 0.7129\n",
      "Epoch 46/60\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.8460 - val_loss: 0.6394\n",
      "Epoch 47/60\n",
      "51877/51877 [==============================] - 4s 83us/sample - loss: 0.6104 - val_loss: 0.5503\n",
      "Epoch 48/60\n",
      "51877/51877 [==============================] - 5s 87us/sample - loss: 0.5907 - val_loss: 0.4535\n",
      "Epoch 49/60\n",
      "51877/51877 [==============================] - 4s 84us/sample - loss: 0.8959 - val_loss: 0.6511\n",
      "Epoch 50/60\n",
      "51877/51877 [==============================] - 6s 108us/sample - loss: 0.7772 - val_loss: 0.4027\n",
      "Epoch 51/60\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.6135 - val_loss: 0.4189\n",
      "Epoch 52/60\n",
      "51877/51877 [==============================] - 4s 79us/sample - loss: 0.6075 - val_loss: 0.7475\n",
      "Epoch 53/60\n",
      "51877/51877 [==============================] - 5s 89us/sample - loss: 0.9781 - val_loss: 0.7346\n",
      "Epoch 54/60\n",
      "51877/51877 [==============================] - 6s 110us/sample - loss: 0.6891 - val_loss: 0.8393\n",
      "Epoch 55/60\n",
      "51877/51877 [==============================] - 6s 113us/sample - loss: 0.8489 - val_loss: 0.6647\n",
      "Epoch 56/60\n",
      "51877/51877 [==============================] - 4s 85us/sample - loss: 0.7154 - val_loss: 0.7517\n",
      "Epoch 57/60\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.7601 - val_loss: 0.6488\n",
      "Epoch 58/60\n",
      "51877/51877 [==============================] - 4s 82us/sample - loss: 0.8025 - val_loss: 0.5205\n",
      "Epoch 59/60\n",
      "51877/51877 [==============================] - 5s 87us/sample - loss: 0.5943 - val_loss: 1.7203\n",
      "Epoch 60/60\n",
      "51877/51877 [==============================] - 4s 82us/sample - loss: 0.8768 - val_loss: 0.7709\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/60\n",
      "51877/51877 [==============================] - 7s 138us/sample - loss: 31.9418 - val_loss: 9.9022\n",
      "Epoch 2/60\n",
      "51877/51877 [==============================] - 4s 78us/sample - loss: 6.2459 - val_loss: 2.8088\n",
      "Epoch 3/60\n",
      "51877/51877 [==============================] - 5s 91us/sample - loss: 1.5246 - val_loss: 0.3368\n",
      "Epoch 4/60\n",
      "51877/51877 [==============================] - 4s 75us/sample - loss: 0.4095 - val_loss: 0.2743\n",
      "Epoch 5/60\n",
      "51877/51877 [==============================] - 4s 76us/sample - loss: 0.3222 - val_loss: 0.2948\n",
      "Epoch 6/60\n",
      "51877/51877 [==============================] - 4s 77us/sample - loss: 0.2789 - val_loss: 0.2763\n",
      "Epoch 7/60\n",
      "51877/51877 [==============================] - 4s 75us/sample - loss: 0.2637 - val_loss: 0.2033\n",
      "Epoch 8/60\n",
      "51877/51877 [==============================] - 4s 77us/sample - loss: 0.2549 - val_loss: 0.2081\n",
      "Epoch 9/60\n",
      "51877/51877 [==============================] - 4s 77us/sample - loss: 0.2641 - val_loss: 0.2289\n",
      "Epoch 10/60\n",
      "51877/51877 [==============================] - 4s 78us/sample - loss: 0.2829 - val_loss: 0.2207\n",
      "Epoch 11/60\n",
      "51877/51877 [==============================] - 4s 78us/sample - loss: 0.3224 - val_loss: 0.2090\n",
      "Epoch 12/60\n",
      "51877/51877 [==============================] - 4s 76us/sample - loss: 0.3719 - val_loss: 0.2808\n",
      "Epoch 13/60\n",
      "51877/51877 [==============================] - 4s 77us/sample - loss: 0.4568 - val_loss: 0.2619\n",
      "Epoch 14/60\n",
      "51877/51877 [==============================] - 4s 79us/sample - loss: 0.5481 - val_loss: 0.3070\n",
      "Epoch 15/60\n",
      "51877/51877 [==============================] - 4s 79us/sample - loss: 0.7226 - val_loss: 0.2628\n",
      "Epoch 16/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51877/51877 [==============================] - 4s 73us/sample - loss: 0.9435 - val_loss: 0.4638\n",
      "Epoch 17/60\n",
      "51877/51877 [==============================] - 4s 74us/sample - loss: 1.2180 - val_loss: 1.7762\n",
      "Epoch 18/60\n",
      "51877/51877 [==============================] - 4s 75us/sample - loss: 1.3971 - val_loss: 0.6393\n",
      "Epoch 19/60\n",
      "51877/51877 [==============================] - 4s 75us/sample - loss: 1.6490 - val_loss: 2.1938\n",
      "Epoch 20/60\n",
      "51877/51877 [==============================] - 4s 75us/sample - loss: 2.0206 - val_loss: 1.4423\n",
      "Epoch 21/60\n",
      "51877/51877 [==============================] - 4s 74us/sample - loss: 2.4549 - val_loss: 0.8583\n",
      "Epoch 22/60\n",
      "51877/51877 [==============================] - 4s 74us/sample - loss: 2.6569 - val_loss: 5.8038\n",
      "Epoch 23/60\n",
      "51877/51877 [==============================] - 4s 75us/sample - loss: 3.0707 - val_loss: 2.7336\n",
      "Epoch 24/60\n",
      "51877/51877 [==============================] - 4s 75us/sample - loss: 3.8567 - val_loss: 0.4068\n",
      "Epoch 25/60\n",
      "51877/51877 [==============================] - 4s 73us/sample - loss: 4.0558 - val_loss: 4.0859\n",
      "Epoch 26/60\n",
      "51877/51877 [==============================] - 4s 74us/sample - loss: 4.6346 - val_loss: 1.3128\n",
      "Epoch 27/60\n",
      "51877/51877 [==============================] - 4s 75us/sample - loss: 5.3454 - val_loss: 4.9979\n",
      "Epoch 28/60\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 5.8531 - val_loss: 0.2711\n",
      "Epoch 29/60\n",
      "51877/51877 [==============================] - 5s 89us/sample - loss: 6.4712 - val_loss: 2.5252\n",
      "Epoch 30/60\n",
      "51877/51877 [==============================] - 4s 75us/sample - loss: 7.2948 - val_loss: 0.4085\n",
      "Epoch 31/60\n",
      "51877/51877 [==============================] - 4s 76us/sample - loss: 8.0925 - val_loss: 1.4458\n",
      "Epoch 32/60\n",
      "51877/51877 [==============================] - 4s 77us/sample - loss: 8.2035 - val_loss: 34.0022\n",
      "Epoch 33/60\n",
      "51877/51877 [==============================] - 4s 82us/sample - loss: 8.8238 - val_loss: 3.4651\n",
      "Epoch 34/60\n",
      "51877/51877 [==============================] - 4s 74us/sample - loss: 9.7662 - val_loss: 31.7105\n",
      "Epoch 35/60\n",
      "51877/51877 [==============================] - 4s 75us/sample - loss: 11.2270 - val_loss: 2.0023\n",
      "Epoch 36/60\n",
      "51877/51877 [==============================] - 4s 76us/sample - loss: 11.1833 - val_loss: 0.2879\n",
      "Epoch 37/60\n",
      "51877/51877 [==============================] - 4s 77us/sample - loss: 11.9217 - val_loss: 6.6362\n",
      "Epoch 38/60\n",
      "51877/51877 [==============================] - 4s 75us/sample - loss: 14.1694 - val_loss: 6.8726\n",
      "Epoch 39/60\n",
      "51877/51877 [==============================] - 4s 75us/sample - loss: 14.3441 - val_loss: 1.3757\n",
      "Epoch 40/60\n",
      "51877/51877 [==============================] - 4s 77us/sample - loss: 14.6017 - val_loss: 64.7914\n",
      "Epoch 41/60\n",
      "51877/51877 [==============================] - 4s 78us/sample - loss: 16.5801 - val_loss: 0.6580\n",
      "Epoch 42/60\n",
      "51877/51877 [==============================] - 4s 77us/sample - loss: 16.7722 - val_loss: 1.5501\n",
      "Epoch 43/60\n",
      "51877/51877 [==============================] - 4s 76us/sample - loss: 18.6946 - val_loss: 0.8053\n",
      "Epoch 44/60\n",
      "51877/51877 [==============================] - 5s 94us/sample - loss: 21.2097 - val_loss: 1.7598\n",
      "Epoch 45/60\n",
      "51877/51877 [==============================] - 4s 77us/sample - loss: 20.5539 - val_loss: 17.6019\n",
      "Epoch 46/60\n",
      "51877/51877 [==============================] - 4s 75us/sample - loss: 23.0563 - val_loss: 37.4017\n",
      "Epoch 47/60\n",
      "51877/51877 [==============================] - 4s 74us/sample - loss: 19.9337 - val_loss: 0.8473\n",
      "Epoch 48/60\n",
      "51877/51877 [==============================] - 4s 75us/sample - loss: 28.4879 - val_loss: 2.3470\n",
      "Epoch 49/60\n",
      "51877/51877 [==============================] - 4s 76us/sample - loss: 24.1308 - val_loss: 40.3690\n",
      "Epoch 50/60\n",
      "51877/51877 [==============================] - 4s 77us/sample - loss: 26.5593 - val_loss: 0.3990\n",
      "Epoch 51/60\n",
      "51877/51877 [==============================] - 4s 76us/sample - loss: 28.6074 - val_loss: 20.7762\n",
      "Epoch 52/60\n",
      "51877/51877 [==============================] - 4s 75us/sample - loss: 27.2979 - val_loss: 58.2876\n",
      "Epoch 53/60\n",
      "51877/51877 [==============================] - 4s 76us/sample - loss: 30.7880 - val_loss: 144.2219\n",
      "Epoch 54/60\n",
      "51877/51877 [==============================] - 4s 77us/sample - loss: 31.8090 - val_loss: 5.6204\n",
      "Epoch 55/60\n",
      "51877/51877 [==============================] - 4s 77us/sample - loss: 37.1408 - val_loss: 10.8727\n",
      "Epoch 56/60\n",
      "51877/51877 [==============================] - 4s 76us/sample - loss: 35.9907 - val_loss: 9.3237\n",
      "Epoch 57/60\n",
      "51877/51877 [==============================] - 4s 78us/sample - loss: 33.0071 - val_loss: 0.9153\n",
      "Epoch 58/60\n",
      "51877/51877 [==============================] - 4s 82us/sample - loss: 37.8131 - val_loss: 8.4817\n",
      "Epoch 59/60\n",
      "51877/51877 [==============================] - 4s 82us/sample - loss: 40.2491 - val_loss: 0.6084\n",
      "Epoch 60/60\n",
      "51877/51877 [==============================] - 5s 87us/sample - loss: 37.8143 - val_loss: 5.9266\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(x_train.shape[1],))\n",
    "hidden1 = Dense(128)(inputs)\n",
    "activ1 = LeakyReLU(0.5)(hidden1)\n",
    "drop1 = Dropout(0.5)(activ1)\n",
    "outputs = Dense(1)(drop1)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=60, validation_data=(x_val, y_val), callbacks=[terminate])\n",
    "\n",
    "inputs = Input(shape=(x_train.shape[1],))\n",
    "hidden1 = Dense(128)(inputs)\n",
    "activ1 = LeakyReLU(0.5)(hidden1)\n",
    "drop1 = Dropout(0.5)(activ1)\n",
    "outputs = Dense(1)(drop1)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer='RMSprop', loss='mse')\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=60, validation_data=(x_val, y_val), callbacks=[terminate])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41SY1xlUG-ld"
   },
   "source": [
    "### 1.m Testing \n",
    "\n",
    "Finalmente, luego de entrenar todos estos modelos estamos en condiciones de probar que tan bien fue nuestro desempeño. Para esto utilice el modelo en el cual obtuvo el mejor desempeño en validación y calcule el error cuadrático medio de la predicción realizada sobre el _Test set_. Para puede utilizar el metodo `.predict` de su modelo. \n",
    "\n",
    "¿Qué tan bien se desempeñaría su modelo en un caso real en vista de lo anterior? Si su curiosidad es suficiente, puede calcular el error real de su modelo transformando nuevamente el _target_ y su predicción a la escala original (utilizando su `scaler`) y tranformando a precio aplicando exponenciación (pues Y estaba espresado en escala logaritmica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3DQ4e89YG-le"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/60\n",
      "51877/51877 [==============================] - 10s 196us/sample - loss: 1.1678 - val_loss: 0.5570\n",
      "Epoch 2/60\n",
      "51877/51877 [==============================] - 5s 106us/sample - loss: 0.5918 - val_loss: 0.2639\n",
      "Epoch 3/60\n",
      "51877/51877 [==============================] - 5s 105us/sample - loss: 0.4787 - val_loss: 0.2290\n",
      "Epoch 4/60\n",
      "51877/51877 [==============================] - 6s 106us/sample - loss: 0.3980 - val_loss: 0.2671\n",
      "Epoch 5/60\n",
      "51877/51877 [==============================] - 6s 106us/sample - loss: 0.3341 - val_loss: 0.1917\n",
      "Epoch 6/60\n",
      "51877/51877 [==============================] - 5s 102us/sample - loss: 0.2956 - val_loss: 0.1915\n",
      "Epoch 7/60\n",
      "51877/51877 [==============================] - 6s 117us/sample - loss: 0.2688 - val_loss: 0.1897\n",
      "Epoch 8/60\n",
      "51877/51877 [==============================] - 6s 107us/sample - loss: 0.2523 - val_loss: 0.1813\n",
      "Epoch 9/60\n",
      "51877/51877 [==============================] - 5s 105us/sample - loss: 0.2331 - val_loss: 0.1942\n",
      "Epoch 10/60\n",
      "51877/51877 [==============================] - 5s 103us/sample - loss: 0.2191 - val_loss: 0.1789\n",
      "Epoch 11/60\n",
      "51877/51877 [==============================] - 5s 106us/sample - loss: 0.2082 - val_loss: 0.1696\n",
      "Epoch 12/60\n",
      "51877/51877 [==============================] - 5s 105us/sample - loss: 0.1968 - val_loss: 0.1731\n",
      "Epoch 13/60\n",
      "51877/51877 [==============================] - 5s 105us/sample - loss: 0.1890 - val_loss: 0.1707\n",
      "Epoch 14/60\n",
      "51877/51877 [==============================] - 6s 108us/sample - loss: 0.1793 - val_loss: 0.1673\n",
      "Epoch 15/60\n",
      "51877/51877 [==============================] - 6s 114us/sample - loss: 0.1727 - val_loss: 0.1677\n",
      "Epoch 16/60\n",
      "51877/51877 [==============================] - 6s 123us/sample - loss: 0.1663 - val_loss: 0.1693\n",
      "Epoch 17/60\n",
      "51877/51877 [==============================] - 6s 113us/sample - loss: 0.1603 - val_loss: 0.1689\n",
      "Epoch 18/60\n",
      "51877/51877 [==============================] - 6s 110us/sample - loss: 0.1542 - val_loss: 0.1711\n",
      "Epoch 19/60\n",
      "51877/51877 [==============================] - 7s 138us/sample - loss: 0.1489 - val_loss: 0.1807\n",
      "Train on 51877 samples, validate on 7559 samples\n",
      "Epoch 1/120\n",
      "51877/51877 [==============================] - 5s 100us/sample - loss: 3.5527 - val_loss: 1.0465\n",
      "Epoch 2/120\n",
      "51877/51877 [==============================] - 4s 77us/sample - loss: 1.0140 - val_loss: 0.3430\n",
      "Epoch 3/120\n",
      "51877/51877 [==============================] - 4s 72us/sample - loss: 0.5264 - val_loss: 0.2823\n",
      "Epoch 4/120\n",
      "51877/51877 [==============================] - 4s 82us/sample - loss: 0.3597 - val_loss: 0.2764\n",
      "Epoch 5/120\n",
      "51877/51877 [==============================] - 4s 77us/sample - loss: 0.3506 - val_loss: 0.2258\n",
      "Epoch 6/120\n",
      "51877/51877 [==============================] - 4s 73us/sample - loss: 0.2967 - val_loss: 0.2114\n",
      "Epoch 7/120\n",
      "51877/51877 [==============================] - 4s 74us/sample - loss: 0.2624 - val_loss: 0.2032\n",
      "Epoch 8/120\n",
      "51877/51877 [==============================] - 4s 78us/sample - loss: 0.2608 - val_loss: 0.2090\n",
      "Epoch 9/120\n",
      "51877/51877 [==============================] - 4s 71us/sample - loss: 0.2533 - val_loss: 0.1992\n",
      "Epoch 10/120\n",
      "51877/51877 [==============================] - 4s 77us/sample - loss: 0.2504 - val_loss: 0.1906\n",
      "Epoch 11/120\n",
      "51877/51877 [==============================] - 5s 92us/sample - loss: 0.2409 - val_loss: 0.1894\n",
      "Epoch 12/120\n",
      "51877/51877 [==============================] - 4s 83us/sample - loss: 0.2402 - val_loss: 0.1935\n",
      "Epoch 13/120\n",
      "51877/51877 [==============================] - 4s 78us/sample - loss: 0.2343 - val_loss: 0.1889\n",
      "Epoch 14/120\n",
      "51877/51877 [==============================] - 4s 81us/sample - loss: 0.2318 - val_loss: 0.1866\n",
      "Epoch 15/120\n",
      "51877/51877 [==============================] - 4s 83us/sample - loss: 0.2334 - val_loss: 0.1864\n",
      "Epoch 16/120\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.2296 - val_loss: 0.1874\n",
      "Epoch 17/120\n",
      "51877/51877 [==============================] - 4s 79us/sample - loss: 0.2280 - val_loss: 0.1880\n",
      "Epoch 18/120\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.2296 - val_loss: 0.1858\n",
      "Epoch 19/120\n",
      "51877/51877 [==============================] - 4s 77us/sample - loss: 0.2242 - val_loss: 0.1854\n",
      "Epoch 20/120\n",
      "51877/51877 [==============================] - 4s 83us/sample - loss: 0.2257 - val_loss: 0.1847\n",
      "Epoch 21/120\n",
      "51877/51877 [==============================] - 4s 81us/sample - loss: 0.2237 - val_loss: 0.1834\n",
      "Epoch 22/120\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.2230 - val_loss: 0.1839\n",
      "Epoch 23/120\n",
      "51877/51877 [==============================] - 4s 79us/sample - loss: 0.2254 - val_loss: 0.1862\n",
      "Epoch 24/120\n",
      "51877/51877 [==============================] - 4s 79us/sample - loss: 0.2239 - val_loss: 0.1847\n",
      "Epoch 25/120\n",
      "51877/51877 [==============================] - 4s 80us/sample - loss: 0.2226 - val_loss: 0.1831\n",
      "Epoch 26/120\n",
      "51877/51877 [==============================] - 6s 107us/sample - loss: 0.2227 - val_loss: 0.1845\n",
      "test MSE modelo deep: 2.8622257259792154\n",
      "test MSE lr decay: 1.171048666917262\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "inputs = Input(shape=(x_train.shape[1],))\n",
    "hidden1 = Dense(256, activation='relu')(inputs)\n",
    "drop1 = Dropout(0.2)(hidden1)\n",
    "hidden2 = Dense(128, activation='relu')(drop1)\n",
    "drop2 = Dropout(0.2)(hidden2)\n",
    "hidden3 = Dense(64, activation='relu')(drop2)\n",
    "drop3 = Dropout(0.2)(hidden3)\n",
    "outputs = Dense(1)(drop3)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "adam = Adam(lr=0.0005)\n",
    "model.compile(optimizer=adam, loss='mse')\n",
    "\n",
    "stopper = EarlyStopping(min_delta=0.001, patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=60, validation_data=(x_val, y_val), callbacks=[stopper, terminate])\n",
    "\n",
    "y_pred1 = model.predict(X_test)\n",
    "\n",
    "inputs = Input(shape=(x_train.shape[1],))\n",
    "hidden1 = Dense(128)(inputs)\n",
    "activ1 = LeakyReLU(0.5)(hidden1)\n",
    "drop1 = Dropout(0.5)(activ1)\n",
    "outputs = Dense(1)(drop1)\n",
    "model2 = Model(inputs=inputs, outputs=outputs)\n",
    "model2.compile(optimizer=SGD(lr=0.001), loss='mse')\n",
    "\n",
    "stopper2 = EarlyStopping(min_delta=0.0005, patience=7, restore_best_weights=True)\n",
    "\n",
    "history = model2.fit(x_train, y_train, epochs=120, validation_data=(x_val, y_val), callbacks=[schedule, terminate, stopper])\n",
    "\n",
    "y_pred2 = model2.predict(X_test)\n",
    "\n",
    "print('test MSE modelo deep:', mean_squared_error(y_pred1, y_test))\n",
    "print('test MSE lr decay:', mean_squared_error(y_pred2, y_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "RYOwa3NMG-kw",
    "tdrih9MOG-k1",
    "-9Z2j4uiG-k8",
    "x3-T18tkG-lC",
    "72-JP1ieG-lH",
    "bHdXHEiXG-lN",
    "41SY1xlUG-ld",
    "7vAU5ESqG-lk",
    "m41rBXEWG-ls",
    "41x3sGUlG-lw",
    "ykxYDM3TG-lz",
    "2_v7x0tVG-l4",
    "vrkhv6tvG-l8",
    "56vV2peEG-mA",
    "SQwUxopqG-mE",
    "AGRqfQ6uG-mJ",
    "BEh44T_dG-mL",
    "VNsonS6_G-mR",
    "_DTu0O8PG-mV",
    "3Is-TnidG-md",
    "C1jRFYieG-md",
    "6tGctPgRG-mf",
    "SrIOqypNG-mk",
    "_cqspYE0G-ml"
   ],
   "name": "Enunciado Tarea 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
